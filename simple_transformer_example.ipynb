{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQeQRZBwUYdTLbifU5IF2c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kla55/transformer/blob/main/simple_transformer_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qiR9QLwi_2x",
        "outputId": "09b4c72a-9882-4466-82a3-6d6d61f90402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "2A0jwZqfjAf_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_prediction(input_text):\n",
        "  # Load pretrained BERT model and tokenizer\n",
        "  model_name = 'bert-base-uncased'\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # Tokenized the input text\n",
        "  inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "  # Get the logits from the model\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "  # Extract logits and fine the predition for the [MASK] token\n",
        "  logits = outputs.logits\n",
        "  mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "  predicted_token_id = logits[0, mask_token_index].argmax(dim=-1).item()\n",
        "  prediction_token = tokenizer.decode(predicted_token_id)\n",
        "  return prediction_token\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6r0ZHZ8TnIKm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The capital of France is [MASK].\"\n",
        "predicted_token = model_prediction(input_text)\n",
        "print(f\"Original sentence: {input_text}\")\n",
        "print(f\"Predicted sentence: The capital of Malaysia is {predicted_token}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9ULbIo6neNi",
        "outputId": "017e2fa1-49c8-4277-acba-c2675f1fad66"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: The capital of France is [MASK].\n",
            "Predicted sentence: The capital of Malaysia is paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"1 + 1 = [MASK].\"\n",
        "predicted_token = model_prediction(input_text)\n",
        "print(f\"Original sentence: {input_text}\")\n",
        "print(f\"Predicted sentence: 1 + 1 =  {predicted_token}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYdpjasBnpHh",
        "outputId": "1c721ba2-e049-44bf-b407-d0f8465a803a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: 1 + 1 = [MASK].\n",
            "Predicted sentence: 1 + 1 =  0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSCoRXm6ocf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LrZ08AxQocoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Custom dataset: country-capital pairs\n",
        "# data = [\n",
        "#     {\"text\": \"The capital of Malaysia is [MASK].\", \"label\": \"Kuala Lumpur\"},\n",
        "#     {\"text\": \"The capital of France is [MASK].\", \"label\": \"Paris\"},\n",
        "#     {\"text\": \"The capital of Japan is [MASK].\", \"label\": \"Tokyo\"},\n",
        "#     {\"text\": \"The capital of India is [MASK].\", \"label\": \"New Delhi\"},\n",
        "# ]\n",
        "data = [\n",
        "    {\"text\": \"The capital of Malaysia is [MASK].\", \"label\": \"Kuala Lumpur\"},\n",
        "    {\"text\": \"Kuala Lumpur is the capital city of Malaysia.\", \"label\": \"Kuala Lumpur\"},\n",
        "    {\"text\": \"Malaysia's capital is [MASK].\", \"label\": \"Kuala Lumpur\"},\n",
        "    {\"text\": \"Paris is the capital of France. The capital of Malaysia is [MASK].\", \"label\": \"Kuala Lumpur\"},\n",
        "]\n",
        "# Save as a DataFrame (for simplicity)\n",
        "df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "HoFZkZYwocra"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom PyTorch Dataset\n",
        "class MaskedLMDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        label = self.data[idx][\"label\"]\n",
        "\n",
        "        # Tokenize the input text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Tokenize the label\n",
        "        label_id = self.tokenizer.convert_tokens_to_ids(label)\n",
        "\n",
        "        # Find the index of the [MASK] token\n",
        "        mask_token_index = (encoding[\"input_ids\"] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "        # Set up labels\n",
        "        labels = torch.full_like(encoding[\"input_ids\"], -100)  # Ignore all tokens except [MASK]\n",
        "        labels[0, mask_token_index] = label_id  # Assign the label token ID to [MASK]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": labels.squeeze(0),\n",
        "        }\n",
        "\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(tokenizer.mask_token_id)\n",
        "print(tokenizer.mask_token)\n",
        "# Create the dataset\n",
        "dataset = MaskedLMDataset(data, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBZzmCC9oe7T",
        "outputId": "ea369e98-d73c-4d6a-c203-6f71057686d4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103\n",
            "[MASK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLXkO5jhol7a",
        "outputId": "ab911005-084f-4bbd-c941-9a472b06fff5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information on the transformer:\n",
        "A **scheduler** in the context of deep learning and training models refers to a mechanism that adjusts the learning rate during training. The learning rate (LR) determines how much the model’s weights are updated during each step of training. A learning rate scheduler helps manage this rate over time, usually starting with a higher rate and gradually decreasing it to stabilize training.\n",
        "\n",
        "**get_scheduler()**: This function creates a learning rate scheduler. The scheduler takes the optimizer and adjusts the learning rate over training steps.\n",
        "\n",
        "**\"linear\"**: This is the type of learning rate schedule. In this case, it's a linear scheduler. A **linear scheduler** gradually decreases the learning rate from a maximum value to a minimum value over a set number of training steps (typically during the course of the entire training). The learning rate decreases linearly.**\n",
        "\n",
        "**optimizer=optimizer**: This refers to the optimizer object that was used to train the model (e.g., Adam, AdamW). The scheduler modifies the learning rate that the optimizer uses during training.\n",
        "\n",
        "# Why AdamW\n",
        "Decoupling Weight decay\\\n",
        "**In Adam:** The weight decay term is applied to the gradient during the update:\\\n",
        "gradients\n",
        "=\n",
        "original gradients\n",
        "+\n",
        "weight decay term\n",
        "gradients=original gradients+weight decay term\n",
        "\n",
        "**In AdamW:** The weight decay term is applied separately from the gradient update:\\\n",
        "new parameter\n",
        "=\n",
        "old parameter\n",
        "−\n",
        "learning rate\n",
        "×\n",
        "(\n",
        "gradient\n",
        "+\n",
        "weight decay term\n",
        ")\n",
        "new parameter=old parameter−learning rate×(gradient+weight decay term)"
      ],
      "metadata": {
        "id": "Nwjc5x7N6pT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Scheduler (optional, e.g., linear decay)\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_training_steps = len(dataloader) * 3  # 3 epochs\n",
        "num_warmup_steps = 1\n",
        "#\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n"
      ],
      "metadata": {
        "id": "CTMKofzho5Lc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Move data to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Average loss: {avg_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af1bEEeoo7td",
        "outputId": "e896e4f3-f70b-4b6a-e72c-acfd3fcf1e74"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Average loss: 10.95367431640625\n",
            "Epoch 2/3\n",
            "Average loss: 10.622824668884277\n",
            "Epoch 3/3\n",
            "Average loss: 0.5249819159507751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fine-tuned model\n",
        "model.eval()\n",
        "input_text = \"The capital of Malaysia is [MASK].\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "logits = outputs.logits\n",
        "mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "predicted_token_id = logits[0, mask_token_index].argmax(dim=-1).item()\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(f\"Original sentence: {input_text}\")\n",
        "print(f\"Predicted sentence: The capital of Malaysia is {predicted_token}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWhW0vrZpedz",
        "outputId": "6d59c87b-d295-4333-a557-f03dd2505794"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: The capital of Malaysia is [MASK].\n",
            "Predicted sentence: The capital of Malaysia is [UNK].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding kuala lumpur into the vocab tokenizer\n",
        "\n",
        "### Key Takeaways\n",
        "- Why Random Initialization?: New tokens are not part of the pre-trained vocabulary, so their embeddings need to be initialized randomly before they can be trained.\n",
        "- How It Works: We resize the embedding layer, then manually initialize the embeddings for the new tokens.\n",
        "- Why Slicing: The slicing operation ensures that we only modify the embeddings for the new tokens added to the vocabulary."
      ],
      "metadata": {
        "id": "oiKUZlxN3nPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add \"Kuala Lumpur\" to tokenizer vocabulary\n",
        "new_tokens = [\"Kuala Lumpur\"]\n",
        "tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "# Resize the model embedding to match the new vocabulary size\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "# manual initialization of the embedding weights for the newly added tokens.\n",
        "# model.get_input_embeddings() - This function returns the input embedding layer of the model.\n",
        "# weight.data This refers to the actual weights of the embedding layer (i.e., the embedding vectors for each token). .data accesses the raw tensor containing these weights.\n",
        "# [-len(new_tokens):] - This slice operation selects the weights corresponding to the new tokens that were added to the tokenizer.\n",
        "# The slice [-len(new_tokens):] selects the last len(new_tokens) rows of the embedding weight matrix. This corresponds to the newly added tokens, as they are typically added at the end of the vocabulary.\n",
        "# torch.randn(len(new_tokens), model.config.hidden_size) - This creates a tensor of shape (len(new_tokens), model.config.hidden_size) filled with random numbers drawn from a normal distribution with mean 0 and variance 1.\n",
        "# model.config.hidden_size refers to the dimensionality of the token embeddings (e.g., for BERT-base, this is 768).\n",
        "\n",
        "#When you add new tokens to the tokenizer, the model’s embedding layer needs to have embeddings for those tokens. Since these tokens were not part of the original vocabulary (and hence were not part of the pretraining), their embeddings are not initialized by the pretrained weights. Instead, they are initialized randomly.\n",
        "# By setting the embeddings to random values using torch.randn(), we give the model the opportunity to learn useful embeddings for these tokens during fine-tuning. This helps the model adjust its representations of the new tokens during training.\n",
        "model.get_input_embeddings().weight.data[-len(new_tokens):] = torch.randn(len(new_tokens), model.config.hidden_size)\n",
        "\n",
        "# Now \"Kuala Lumpur\" will not be tokenized as [UNK]\n",
        "print(tokenizer.tokenize(\"Kuala Lumpur\"))\n",
        "\n",
        "dataset = MaskedLMDataset(data, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLPeuioHpoPJ",
        "outputId": "5b9e81ad-7270-499c-b875-488171d85e8d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['kuala lumpur']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"Embedding size: {model.get_input_embeddings().weight.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naJocr7RstRF",
        "outputId": "b84f0a3a-4cc4-4001-c400-ff4a8ef3b0ff"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vocabulary size: 30523\n",
            "Embedding size: torch.Size([30523, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WncQPKCsuZxF",
        "outputId": "e65f4b19-3be3-4138-a404-c57661260c18"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30523, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30523, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(dataloader) * 3 # 3 epochs\n",
        "num_warmup_steps = 1\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "# Fine-tuning loop\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} Loss: {total_loss / len(dataloader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrnJRxIWtIh2",
        "outputId": "1ef4692d-a1a2-41c2-dfd0-a55edbb7d5a8"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Epoch 1 Loss: 8.025346755981445\n",
            "Epoch 2\n",
            "Epoch 2 Loss: 15.467495918273926\n",
            "Epoch 3\n",
            "Epoch 3 Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The capital of Malaysia is [MASK].\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "logits = outputs.logits\n",
        "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "predicted_token_id = logits[0, mask_token_index].argmax(dim=-1).item()\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(f\"Original sentence: {input_text}\")\n",
        "print(f\"Predicted sentence: The capital of Malaysia is {predicted_token}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bsE0GxCrGPp",
        "outputId": "ef289d4f-a545-45d5-b0a3-c22de79e186d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: The capital of Malaysia is [MASK].\n",
            "Predicted sentence: The capital of Malaysia is Kuala Lumpur.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lF-GgFJyxhvH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}