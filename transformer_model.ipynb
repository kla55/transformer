{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kla55/transformer/blob/main/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PcQkM8AMZU3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FtGJMT3CZEg3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The InputEmbeddings class is a PyTorch module that maps input tokens (usually integers representing words or subwords) to high-dimensional vector representations (embeddings). This is a common first step in many neural network models for natural language processing (NLP), such as transformers.\n",
        "\n",
        "Key Components\n",
        "Constructor (__init__):\\\n",
        "d_model: Dimensionality of the embedding vectors. Each token will be mapped to a vector of size d_model.\\\n",
        "vocab_size: Number of unique tokens in the vocabulary. This defines the size of the embedding matrix, which will have shape (vocab_size, d_model).\\\n",
        "Embedding Layer (nn.Embedding):\n",
        "Maps integers (token indices) to dense vectors of size d_model.\n",
        "The embedding layer is a trainable lookup table with dimensions (vocab_size, d_model), where each row corresponds to the vector representation of a token.\\\n",
        "Forward Pass (forward):\n",
        "- Input:\n",
        "  - x: A tensor of token indices (shape: (batch_size, seq_len)).\n",
        "- Process:\n",
        "  - self.embedding(x): Looks up the embeddings for the tokens in x, resulting in a tensor of shape (batch_size, seq_len, d_model).\n",
        "  - math.sqrt(self.d_model): Scales the embeddings by the square root of d_model to ensure the embeddings have a consistent magnitude, which stabilizes training. This is a common technique from the Transformer architecture.\n",
        "- Output:\n",
        "  - A scaled tensor of embeddings with shape (batch_size, seq_len, d_model).\n",
        "Why Multiply by\n",
        "ùëëùëöùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        "d\n",
        "model\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        " ?\n",
        "Multiplying by\n",
        "ùëë\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        "d\n",
        "model\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "  (the square root of the embedding size) is a normalization technique used in the original Transformer paper (Attention Is All You Need). Without scaling, the variance of the embeddings might grow too large or too small, especially when combined with positional encodings. This scaling ensures embeddings are appropriately weighted when passed into subsequent layers."
      ],
      "metadata": {
        "id": "zAgDccDGn6KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int): # constructor - needs dimensions and vocab size\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model) # mapping between numbers and vector size - 512\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model) # sqrt"
      ],
      "metadata": {
        "id": "rJ5-04bLZaVg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "d_model = 512\n",
        "vocab_size = 10000\n",
        "input_embeddings = InputEmbeddings(d_model=d_model, vocab_size=vocab_size)\n",
        "\n",
        "# Sample input: batch of sequences with token IDs\n",
        "sample_input = torch.randint(0, vocab_size, (4, 10))  # Batch of 4 sequences, each with 10 tokens\n",
        "embedded_output = input_embeddings(sample_input)\n",
        "\n",
        "print(\"Input shape:\", sample_input.shape)  # (batch_size, sequence_length)\n",
        "print(\"Output shape:\", embedded_output.shape)  # (batch_size, sequence_length, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLeAeUIDcoCj",
        "outputId": "a323c6c4-0adc-4fb6-9f4a-ee5bae84e4d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 10])\n",
            "Output shape: torch.Size([4, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PositionalEncoding class is designed to inject positional information into token embeddings. In the Transformer architecture, positional encodings are crucial because the model itself does not inherently understand sequence order (unlike recurrent networks). This implementation follows the approach described in the paper \"Attention Is All You Need\".\n",
        "\n",
        "Key Components of the Code\\\n",
        "Constructor (__init__):\\\n",
        "Inputs:\\\n",
        "d_model: Dimensionality of embeddings (vector size for each token).\\\n",
        "seq_len: Maximum sequence length for which positional encodings are precomputed.\\\n",
        "dropout: Dropout probability for regularization.\\\n",
        "Initialization:\\\n",
        "pe: Precomputed positional encodings, stored as a tensor with shape (1, seq_len, d_model).\\\n",
        "position: A tensor representing positions from 0 to seq_len - 1.\\\n",
        "div_term: A scaling factor to adjust frequencies for the sinusoidal functions.\\\n",
        "Sinusoidal Encoding:\\\n",
        "Even indices (\n",
        "2\n",
        "ùëñ\n",
        "2i) use sine (\n",
        "sin\n",
        "‚Å°\n",
        "sin).\n",
        "Odd indices (\n",
        "2\n",
        "ùëñ\n",
        "+\n",
        "1\n",
        "2i+1) use cosine (\n",
        "cos\n",
        "‚Å°\n",
        "cos).\n",
        "register_buffer: Stores the precomputed positional encodings in the model without treating them as learnable parameters.\\\n",
        "Forward Pass (forward):\\\n",
        "Inputs:\\\n",
        "x: Input tensor (e.g., token embeddings) with shape (batch_size, seq_len, d_model).\\\n",
        "Process:\\\n",
        "Adds the positional encoding tensor (pe) to the input embeddings. The encoding values corresponding to the input's sequence length (x.size(1)) are sliced from the precomputed encodings.\\\n",
        ".requires_grad_(False) ensures positional encodings are not updated during backpropagation.\\\n",
        "Applies dropout to prevent overfitting.\\\n",
        "Outputs:\\\n",
        "Tensor of the same shape as x ((batch_size, seq_len, d_model)), with positional information added.\\\n",
        "Mathematics of Positional Encoding\\\n",
        "The encoding for a position \\\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        "pos and dimension\n",
        "ùëñ\n",
        "i is computed as:\n",
        "\n",
        "ùëÉ\n",
        "ùê∏\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        ",\n",
        "2\n",
        "ùëñ\n",
        "=\n",
        "sin\n",
        "‚Å°\n",
        "(\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        "1000\n",
        "0\n",
        "2\n",
        "ùëñ\n",
        "/\n",
        "ùëë\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        ")\n",
        "PE\n",
        "pos,2i\n",
        "‚Äã\n",
        " =sin(\n",
        "10000\n",
        "2i/d\n",
        "model\n",
        "‚Äã\n",
        "\n",
        "\n",
        "pos\n",
        "‚Äã\n",
        " )\n",
        "ùëÉ\n",
        "ùê∏\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        ",\n",
        "2\n",
        "ùëñ\n",
        "+\n",
        "1\n",
        "=\n",
        "cos\n",
        "‚Å°\n",
        "(\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        "1000\n",
        "0\n",
        "2\n",
        "ùëñ\n",
        "/\n",
        "ùëë\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        ")\n",
        "PE\n",
        "pos,2i+1\n",
        "‚Äã\n",
        " =cos(\n",
        "10000\n",
        "2i/d\n",
        "model\n",
        "‚Äã\n",
        "\n",
        "\n",
        "pos\n",
        "‚Äã\n",
        " )\n",
        "Even Dimensions (\n",
        "2\n",
        "ùëñ\n",
        "2i): Use sine.\n",
        "Odd Dimensions (\n",
        "2\n",
        "ùëñ\n",
        "+\n",
        "1\n",
        "2i+1): Use cosine.\n",
        "The factor\n",
        "1000\n",
        "0\n",
        "2\n",
        "ùëñ\n",
        "/\n",
        "ùëë\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        "10000\n",
        "2i/d\n",
        "model\n",
        "‚Äã\n",
        "\n",
        "  ensures that the frequencies of sine and cosine functions differ across dimensions, providing unique positional information.\\\n",
        "Why Sinusoidal Encodings?\\\n",
        "Continuity: Allows the model to extrapolate to sequence lengths beyond those seen during training.\\\n",
        "Unique Encodings: Each position and dimension gets a unique encoding.\n",
        "Relative Position: The difference between encodings of positions conveys the relative positional information, which is useful for attention mechanisms."
      ],
      "metadata": {
        "id": "SdBlv9mJriX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # create an array\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        print(pe.shape)\n",
        "        # create a position tensor\n",
        "        # - Adds an additional dimension to the tensor at index 1, converting the 1D tensor into a 2D tensor - (seq_len, 1)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        print(position.shape)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        print(div_term.shape)\n",
        "        # apply the sin to even positions and cos to odd positions\n",
        "        # pe[all vocab, starting at position 0/1 and for every 2]\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0) # (seq_len, d_model) -> (1, seq_len, d_model)\n",
        "        # A buffer is a persistent tensor in the model that is not considered a learnable parameter (i.e., it won't be updated during backpropagation).\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # to add this positional encoding to every word inside the sentence\n",
        "        # extracts the part of the positional encoding needed for the current input and locks it so it won't change during training\n",
        "        # :x.size(1): Selects elements up to the length of the sequence dimension of x. This means that the operation is selecting a subset of self.pe that matches the sequence length of the input tensor x.\n",
        "        x = x + (self.pe[:, :x.size(1), :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "IGmmxMf2at5Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "d_model = 512\n",
        "seq_len = 10\n",
        "dropout = 0.1\n",
        "pos_encoding = PositionalEncoding(d_model=d_model, seq_len=seq_len, dropout=dropout)\n",
        "\n",
        "# Sample input: batch of embeddings\n",
        "batch_size = 4\n",
        "sample_embeddings = torch.randn(batch_size, seq_len, d_model)  # Random embeddings for a batch of 4 sequences\n",
        "\n",
        "# Apply positional encoding\n",
        "encoded_output = pos_encoding(sample_embeddings)\n",
        "\n",
        "print(\"Input shape:\", sample_embeddings.shape)  # (batch_size, seq_len, d_model)\n",
        "print(\"Output shape:\", encoded_output.shape)     # (batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNDvThqTfKO6",
        "outputId": "3a2263c8-ba40-433c-a3ef-31c174d643a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 512])\n",
            "torch.Size([10, 1])\n",
            "torch.Size([256])\n",
            "Input shape: torch.Size([4, 10, 512])\n",
            "Output shape: torch.Size([4, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LayerNormalization class implements a custom layer normalization module, which is commonly used in neural networks, especially in architectures like Transformers. Layer normalization normalizes inputs across the features for each individual data point, ensuring that the mean is 0 and the standard deviation is 1. This helps stabilize and accelerate training.\n",
        "\n",
        "Key Components of the Code\\\n",
        "Constructor (__init__):\n",
        "\n",
        "eps:\\\n",
        "A small value (epsilon) added to the denominator to prevent division by zero during standard deviation computation.\\\n",
        "Default is set to\n",
        "1\n",
        "0\n",
        "6\n",
        "10\n",
        "6\n",
        " , which is an unusually large value. A typical choice for eps is\n",
        "1\n",
        "0\n",
        "‚àí\n",
        "5\n",
        "10\n",
        "‚àí5\n",
        "  or\n",
        "1\n",
        "0\n",
        "‚àí\n",
        "6\n",
        "10\n",
        "‚àí6\n",
        " . This might be a typo or require re-evaluation based on the use case.\\\n",
        "alpha:\\\n",
        "A learnable parameter (initialized to 1) that scales the normalized values.\\\n",
        "beta:\\\n",
        "A learnable parameter (initialized to 0) that shifts the normalized values.\\\n",
        "Forward Pass (forward):\\\n",
        "\n",
        "Inputs:\\\n",
        "x: The input tensor, typically of shape (batch_size, seq_len, d_model) or (batch_size, features) for layer normalization.\\\n",
        "Process:\n",
        "Compute the mean and standard deviation across the last dimension (\n",
        "‚àí\n",
        "1\n",
        "‚àí1), which corresponds to features in the tensor.\n",
        "Normalize the input\n",
        "ùë•\n",
        "x as:\n",
        "Normalized\n",
        "ùë•\n",
        "=\n",
        "ùë•\n",
        "‚àí\n",
        "mean\n",
        "std\n",
        "+\n",
        "eps\n",
        "Normalized¬†x=\n",
        "std+eps\n",
        "x‚àímean\n",
        "‚Äã\n",
        "\n",
        "Scale and shift the normalized tensor using alpha and beta:\n",
        "Output\n",
        "=\n",
        "ùõº\n",
        "‚ãÖ\n",
        "Normalized\n",
        "ùë•\n",
        "+\n",
        "ùõΩ\n",
        "Output¬†=Œ±‚ãÖNormalized¬†x+Œ≤\n",
        "Outputs:\n",
        "A tensor of the same shape as the input, with normalized values.\n",
        "Key Features\n",
        "Learnable Parameters (alpha and beta):\n",
        "\n",
        "These allow the network to \"denormalize\" the values if needed by learning appropriate scaling and shifting.\n",
        "Normalization Across Features:\n",
        "\n",
        "Layer normalization operates on the last dimension (\n",
        "‚àí\n",
        "1\n",
        "‚àí1) for each data point in a batch. This is different from batch normalization, which normalizes across the batch dimension.\n",
        "Mathematical Representation\n",
        "For an input tensor\n",
        "ùë•\n",
        "x, the output is computed as:\n",
        "\n",
        "Output\n",
        "ùëñ\n",
        ",\n",
        "ùëó\n",
        "=\n",
        "ùõº\n",
        "‚ãÖ\n",
        "ùë•\n",
        "ùëñ\n",
        ",\n",
        "ùëó\n",
        "‚àí\n",
        "ùúá\n",
        "ùëó\n",
        "ùúé\n",
        "ùëó\n",
        "+\n",
        "ùúñ\n",
        "+\n",
        "ùõΩ\n",
        "Output\n",
        "i,j\n",
        "‚Äã\n",
        " =Œ±‚ãÖ\n",
        "œÉ\n",
        "j\n",
        "‚Äã\n",
        " +œµ\n",
        "x\n",
        "i,j\n",
        "‚Äã\n",
        " ‚àíŒº\n",
        "j\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        " +Œ≤\n",
        "Where:\n",
        "\n",
        "ùëñ\n",
        "i: Index over batch or sequence.\n",
        "ùëó\n",
        "j: Index over features.\n",
        "ùúá\n",
        "ùëó\n",
        "Œº\n",
        "j\n",
        "‚Äã\n",
        " : Mean of the\n",
        "ùëó\n",
        "ùë°\n",
        "‚Ñé\n",
        "j\n",
        "th\n",
        "  feature across the batch/sequence.\n",
        "ùúé\n",
        "ùëó\n",
        "œÉ\n",
        "j\n",
        "‚Äã\n",
        " : Standard deviation of the\n",
        "ùëó\n",
        "ùë°\n",
        "‚Ñé\n",
        "j\n",
        "th\n",
        "  feature across the batch/sequence."
      ],
      "metadata": {
        "id": "qYHUuU_MKkQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  \"\"\"\n",
        "  Layer normalization is a technique used in neural networks to stabilize and accelerate the training process.\n",
        "  It normalizes the inputs across the features of each layer, which helps in making the model more robust and easier to train.\n",
        "\n",
        "  In layer normalization, the mean and variance are computed for each individual sample across all the features (or neurons) within a layer.\n",
        "  The input to a particular layer is normalized by subtracting the mean and dividing by the standard deviation calculated over the features of that input. This results in inputs that have a mean of 0 and a standard deviation of 1.\n",
        "  After normalization, the output is typically scaled and shifted using learnable parameters (gamma and beta) so that the network can still represent a wide range of inputs if needed.\n",
        "  Need Epislon for stability - if sigma is close to 0 then the mew value becomes big - so we do not want big or small values\n",
        "  \"\"\"\n",
        "  def __init__(self, eps: float = 10**6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.alpha = nn.Parameter(torch.ones(1)) # multiplier\n",
        "    self.beta = nn.Parameter(torch.zeros(1)) # additive\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.alpha * (x - mean) / (std + self.eps)"
      ],
      "metadata": {
        "id": "bSpVIuoXXHfE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalisation(nn.Module):\n",
        "    def __init__(self, epsilon=1e-6):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias"
      ],
      "metadata": {
        "id": "kI00s5EOl_fT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "layer_norm = LayerNormalization(eps=1e-6)\n",
        "\n",
        "# Create a batch of input data\n",
        "input_data = torch.randn(4, 6)  # Batch of 4 samples, each with 6 features\n",
        "\n",
        "# Apply layer normalization\n",
        "normalized_output = layer_norm(input_data)\n",
        "\n",
        "print(\"Input data:\\n\", input_data)\n",
        "print(\"\\nNormalized output:\\n\", normalized_output)\n",
        "print(\"\\nOutput mean (per sample):\", normalized_output.mean(-1))  # Should be close to 0\n",
        "print(\"Output std (per sample):\", normalized_output.std(-1))    # Should be close to 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_zIZN40jj71",
        "outputId": "e6fd986e-98b9-4751-c3b3-f1ea8e9d02db"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:\n",
            " tensor([[ 1.8976, -1.0322,  0.1577, -0.9157,  0.0041, -0.6351],\n",
            "        [-2.0992, -0.7091,  1.4959, -1.3348,  1.0720,  1.0297],\n",
            "        [-0.1621, -0.7507,  1.5683,  1.0657,  0.4236,  1.8767],\n",
            "        [ 0.6132,  2.5309,  0.2125,  1.6992, -0.2915,  0.3005]])\n",
            "\n",
            "Normalized output:\n",
            " tensor([[ 1.8292, -0.8708,  0.2258, -0.7635,  0.0841, -0.5048],\n",
            "        [-1.3486, -0.4151,  1.0655, -0.8353,  0.7809,  0.7525],\n",
            "        [-0.8169, -1.3945,  0.8814,  0.3881, -0.2421,  1.1841],\n",
            "        [-0.2179,  1.5919, -0.5961,  0.8070, -1.0718, -0.5131]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "\n",
            "Output mean (per sample): tensor([ 1.9868e-08, -1.9868e-08, -1.4901e-08, -3.9736e-08],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "Output std (per sample): tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The FeedForwardBlock is a critical component of the Transformer architecture, implementing the Position-Wise Feedforward Network (FFN). Below is a detailed breakdown of what it does:\n",
        "\n",
        "Structure and Purpose\n",
        "Input and Output Dimensions:\n",
        "\n",
        "Input:\n",
        "(Batch,¬†Sequence¬†Length,¬†d_model)\n",
        "(Batch,¬†Sequence¬†Length,¬†d_model)\n",
        "Output:\n",
        "(Batch,¬†Sequence¬†Length,¬†d_model)\n",
        "(Batch,¬†Sequence¬†Length,¬†d_model)\n",
        "The FFN operates independently on each position (or token) in the sequence, transforming features into a higher-dimensional space and back.\n",
        "\n",
        "Components:\n",
        "\n",
        "nn.Linear(d_model, d_ff): Expands features from\n",
        "ùëë\n",
        "_\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        "d_model to\n",
        "ùëë\n",
        "_\n",
        "ùëì\n",
        "ùëì\n",
        "d_ff (hidden layer size).\n",
        "torch.relu: Applies a non-linear activation function.\n",
        "nn.Dropout: Randomly zeroes some elements during training to prevent overfitting.\n",
        "nn.Linear(d_ff, d_model): Projects features back to the original\n",
        "ùëë\n",
        "_\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        "d_model size.\n",
        "Purpose:\n",
        "\n",
        "The block introduces non-linearity and learns complex representations. The intermediate expansion to\n",
        "ùëë\n",
        "_\n",
        "ùëì\n",
        "ùëì\n",
        "d_ff increases the model's capacity.\n",
        "Forward Pass Explanation\n",
        "The forward method performs the following steps:\n",
        "\n",
        "First Linear Transformation:\n",
        "\n",
        "ùë•\n",
        "intermediate\n",
        "=\n",
        "Linear\n",
        "1\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "x\n",
        "intermediate\n",
        "‚Äã\n",
        " =Linear\n",
        "1\n",
        "‚Äã\n",
        " (x)\n",
        "Projects the input\n",
        "ùë•\n",
        "x from\n",
        "ùëë\n",
        "_\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        "d_model to\n",
        "ùëë\n",
        "_\n",
        "ùëì\n",
        "ùëì\n",
        "d_ff.\n",
        "\n",
        "Non-Linearity (ReLU):\n",
        "\n",
        "ùë•\n",
        "relu\n",
        "=\n",
        "ReLU\n",
        "(\n",
        "ùë•\n",
        "intermediate\n",
        ")\n",
        "x\n",
        "relu\n",
        "‚Äã\n",
        " =ReLU(x\n",
        "intermediate\n",
        "‚Äã\n",
        " )\n",
        "Applies element-wise activation.\n",
        "\n",
        "Dropout:\n",
        "\n",
        "ùë•\n",
        "dropped\n",
        "=\n",
        "Dropout\n",
        "(\n",
        "ùë•\n",
        "relu\n",
        ")\n",
        "x\n",
        "dropped\n",
        "‚Äã\n",
        " =Dropout(x\n",
        "relu\n",
        "‚Äã\n",
        " )\n",
        "Randomly zeroes elements to prevent overfitting.\n",
        "\n",
        "Second Linear Transformation:\n",
        "\n",
        "ùë•\n",
        "output\n",
        "=\n",
        "Linear\n",
        "2\n",
        "(\n",
        "ùë•\n",
        "dropped\n",
        ")\n",
        "x\n",
        "output\n",
        "‚Äã\n",
        " =Linear\n",
        "2\n",
        "‚Äã\n",
        " (x\n",
        "dropped\n",
        "‚Äã\n",
        " )\n",
        "Projects the output back to the original dimension\n",
        "ùëë\n",
        "_\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        "d_model."
      ],
      "metadata": {
        "id": "zBExhKr5eQsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (Batch, Sequence, d_model) -> (Batch, Sequence, dff) -> (Batch, Sequence, d_model)\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "id": "XUkrHMoMbZcE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "ff_block = FeedForwardBlock(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "\n",
        "# Input data: batch of 4 sequences, each with 10 tokens and an embedding dimension of 512\n",
        "input_data = torch.randn(4, 10, d_model)\n",
        "\n",
        "# Pass through the feedforward block\n",
        "output_data = ff_block(input_data)\n",
        "\n",
        "print(\"Input shape:\", input_data.shape)  # (Batch, Sequence, d_model)\n",
        "print(\"Output shape:\", output_data.shape)  # Should also be (Batch, Sequence, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E5F3rvV1_QL",
        "outputId": "ba85d5cc-7499-4905-af3b-5a2c1264df2d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 10, 512])\n",
            "Output shape: torch.Size([4, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.attention_scores = None\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.d_k = d_model // self.num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.layer_norm1 = LayerNormalisation()\n",
        "        self.layer_norm2 = LayerNormalisation()\n",
        "        self.layer_norm3 = LayerNormalisation()\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(q, k, v, mask, dropout):\n",
        "        d_k = q.shape[-1]\n",
        "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1)  # (Batch, num_heads, Seq_Len,  Seq_Len)\n",
        "\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        attn = torch.matmul(attention_scores, v)  # (Batch, num_heads, Seq_Len, d_k)\n",
        "        return attn, attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "\n",
        "        q = self.wq(q)\n",
        "\n",
        "        k = self.wk(k)\n",
        "\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = q.view(q.shape[0], q.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        k = k.view(k.shape[0], k.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        v = v.view(v.shape[0], v.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, self.attention_scores = MultiHeadAttention.attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "        x = self.wo(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "29gHj0ocq9ZC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "01ZY73JSflDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MultiHeadAttention class implements the Multi-Head Attention mechanism, which is one of the core components of the Transformer architecture. Here‚Äôs a detailed breakdown of what this class does:\n",
        "\n",
        "Components\n",
        "Parameters:\n",
        "\n",
        "d_model: The dimensionality of input and output feature vectors.\n",
        "num_heads: The number of attention heads. Each head computes attention separately and independently.\n",
        "dropout: Dropout probability to prevent overfitting.\n",
        "Key Features:\n",
        "\n",
        "Multi-head attention mechanism:\n",
        "Projects queries (\n",
        "ùëû\n",
        "q), keys (\n",
        "ùëò\n",
        "k), and values (\n",
        "ùë£\n",
        "v) into multiple subspaces for parallel computation.\n",
        "Linear projections:\n",
        "wq, wk, wv: Linear layers to compute queries, keys, and values.\n",
        "wo: Linear layer to combine outputs of all attention heads.\n",
        "Dropout: Applied to the attention scores for regularization.\n",
        "Layer normalization: Added (though not typical in this exact location in standard implementations).\n",
        "Key Calculation Dimensions:\n",
        "\n",
        "d_k\n",
        "=\n",
        "d_model\n",
        "num_heads\n",
        "d_k=\n",
        "num_heads\n",
        "d_model\n",
        "‚Äã\n",
        " : Dimensionality per attention head.\n",
        "q\n",
        ",\n",
        "k\n",
        ",\n",
        "v\n",
        "q,k,v: Projected tensors reshaped for multi-head computation.\n",
        "Key Functions\n",
        "attention (Static Method):\n",
        "Computes scaled dot-product attention:\n",
        "\n",
        "Attention\n",
        "(\n",
        "ùëÑ\n",
        ",\n",
        "ùêæ\n",
        ",\n",
        "ùëâ\n",
        ")\n",
        "=\n",
        "softmax\n",
        "(\n",
        "ùëÑ\n",
        "ùêæ\n",
        "ùëá\n",
        "ùëë\n",
        "ùëò\n",
        ")\n",
        "ùëâ\n",
        "Attention(Q,K,V)=softmax(\n",
        "d\n",
        "k\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "\n",
        "QK\n",
        "T\n",
        "\n",
        "‚Äã\n",
        " )V\n",
        "Inputs:\n",
        "\n",
        "q: Query tensor (\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†d_k)\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†d_k)).\n",
        "k: Key tensor (\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†d_k)\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†d_k)).\n",
        "v: Value tensor (\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†d_k)\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†d_k)).\n",
        "mask: Tensor to mask certain attention scores (e.g., for padding or causal masking).\n",
        "dropout: Dropout function applied to the attention weights.\n",
        "Outputs:\n",
        "\n",
        "attn: Weighted sum of the values (\n",
        "ùëâ\n",
        "V).\n",
        "attention_scores: Softmaxed attention scores (\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†Seq_Len)\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†Seq_Len)).\n",
        "forward:\n",
        "Input Projections:\n",
        "\n",
        "Applies linear transformations (wq, wk, wv) to input tensors\n",
        "ùëû\n",
        "q,\n",
        "ùëò\n",
        "k,\n",
        "ùë£\n",
        "v to generate queries, keys, and values.\n",
        "Reshaping:\n",
        "\n",
        "Splits the\n",
        "ùëë\n",
        "_\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        "d_model-dimensional vectors into\n",
        "num_heads\n",
        "num_heads subspaces of size\n",
        "ùëë\n",
        "ùëò\n",
        "d\n",
        "k\n",
        "‚Äã\n",
        " , reshaping into:\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†d_k)\n",
        ".\n",
        "(Batch,¬†num_heads,¬†Seq_Len,¬†d_k).\n",
        "Scaled Dot-Product Attention:\n",
        "\n",
        "Calls the attention function, computes attention scores, and applies them to values.\n",
        "Output Reconstruction:\n",
        "\n",
        "Merges the outputs of all attention heads into a single tensor of shape:\n",
        "(Batch,¬†Seq_Len,¬†d_model)\n",
        ".\n",
        "(Batch,¬†Seq_Len,¬†d_model).\n",
        "Final Linear Transformation:\n",
        "\n",
        "Applies wo to map back to\n",
        "d_model\n",
        "d_model-dimensional space.\n"
      ],
      "metadata": {
        "id": "aZOBeAa3fllu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, h: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // h\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "    d_k = query.shape[-1]\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(torch.softmax(attention_scores, dim=-1))\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    # q = [batch size, query len, hid dim]\n",
        "    # k = [batch size, key len, hid dim]\n",
        "    # v = [batch size, value len, hid dim]\n",
        "    query = self.w_q(q)\n",
        "    key = self.w_k(k)\n",
        "    value = self.w_v(v)\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) #.permute(0, 2, 1, 3)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "    x = self.w_o(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "grdHQVW7jyaV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "h = 8\n",
        "dropout = 0.1\n",
        "mha_block = MultiHeadAttentionBlock(d_model=d_model, h=h, dropout=dropout)\n",
        "\n",
        "# Define input tensors for a batch of sequences\n",
        "batch_size = 4\n",
        "sequence_length = 10\n",
        "q = torch.randn(batch_size, sequence_length, d_model)\n",
        "k = torch.randn(batch_size, sequence_length, d_model)\n",
        "v = torch.randn(batch_size, sequence_length, d_model)\n",
        "mask = None  # Example without mask\n",
        "\n",
        "# Pass through the multi-head attention block\n",
        "output = mha_block(q, k, v, mask)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # (Batch, Sequence, d_model)"
      ],
      "metadata": {
        "id": "wlCkhu3u3uWV",
        "outputId": "1e6839e9-6701-4b03-b6f2-abb667c866d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([4, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, dropout: float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.norm(x))) # residual connection"
      ],
      "metadata": {
        "id": "A_NNizN6x-8M"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))"
      ],
      "metadata": {
        "id": "v3p_oykaUeDC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input dimensions\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "d_model = 10\n",
        "d_ff = 20\n",
        "\n",
        "# Instantiate modules\n",
        "dropout_rate = 0.1\n",
        "residual_connection = ResidualConnection(dropout=dropout_rate)\n",
        "feedforward = FeedForwardLayer(d_model=d_model, d_ff=d_ff)\n",
        "# Create a sample input\n",
        "x = torch.rand(batch_size, seq_len, d_model)\n",
        "# Apply ResidualConnection with the FeedForwardLayer as the sublayer\n",
        "output = residual_connection(x, feedforward)\n",
        "\n",
        "print(\"Input Shape:\", x.shape)\n",
        "print(\"Output Shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytHNoGGYTycg",
        "outputId": "11a6eeb5-2060-426e-b526-39f75948a62a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([2, 5, 10])\n",
            "Output Shape: torch.Size([2, 5, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "kns6E0Mzy-nE"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input parameters\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "d_ff = 256\n",
        "dropout = 0.1\n",
        "\n",
        "# Instantiate components\n",
        "self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
        "feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "encoder_block = EncoderBlock(self_attention_block, feed_forward_block, dropout)\n",
        "\n",
        "# Example input\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "x = torch.rand(seq_len, batch_size, d_model)  # Transformer input is (seq_len, batch_size, d_model)\n",
        "src_mask = None  # Example without masking\n",
        "\n",
        "# Forward pass\n",
        "output = encoder_block(x, src_mask)\n",
        "print(\"Input Shape:\", x.shape)\n",
        "print(\"Output Shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilx3yIhtBM1E",
        "outputId": "3d4ee21b-ccc7-4697-850e-84bb7f6506e6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([10, 2, 64])\n",
            "Output Shape: torch.Size([10, 2, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "pdAz2SsC4Lgq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36B2s1qonxUY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example configuration\n",
        "features = 64\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "d_ff = 256\n",
        "self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
        "feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "\n",
        "# Create multiple EncoderBlocks\n",
        "encoder_layers = nn.ModuleList([\n",
        "    EncoderBlock(self_attention_block, feed_forward_block, dropout) for _ in range(num_layers)\n",
        "])\n",
        "\n",
        "# Instantiate the Encoder\n",
        "encoder = Encoder(layers=encoder_layers)\n",
        "\n",
        "# Input tensor (sequence length, batch size, feature size)\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "x = torch.rand(seq_len, batch_size, features)\n",
        "mask = None  # Example without masking\n",
        "\n",
        "# Forward pass\n",
        "output = encoder(x, mask)\n",
        "\n",
        "print(\"Input Shape:\", x.shape)\n",
        "print(\"Output Shape:\", output.shape)"
      ],
      "metadata": {
        "id": "j0Avhi4g4WYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968f2c89-2a79-4653-8ee3-992a9f0bc823"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([10, 2, 64])\n",
            "Output Shape: torch.Size([10, 2, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cEf8wqpiREs2"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff  # Feed Forward Neural Network Output Size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = FeedForward(d_model, dff, dropout)\n",
        "        self.residual_mha = ResidualConnection(dropout)\n",
        "        self.residual_cross_mha = ResidualConnection(dropout)\n",
        "        self.residual_ffn = ResidualConnection(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
        "        # Multi-Head Attention sub-layer\n",
        "        attn_output = self.residual_mha(x, lambda x: self.mha(x, x, x, target_mask))\n",
        "\n",
        "        # Cross-Attention sub-layer\n",
        "        cross_attn_output = self.residual_cross_mha(attn_output,\n",
        "                                                    lambda x: self.mha(x, encoder_output, encoder_output, source_mask))\n",
        "\n",
        "        # FeedForward sub-layer\n",
        "        ffn_output = self.residual_ffn(cross_attn_output, self.ffn)\n",
        "\n",
        "        return ffn_output"
      ],
      "metadata": {
        "id": "i-Ask2r2er69"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"InputEmbeddings - Input x shape:\", x.shape)\n",
        "        embeddings = self.word_embeddings(x) * math.sqrt(self.d_model)\n",
        "        print(\"InputEmbeddings - Output embeddings shape:\", embeddings.shape)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"PositionalEncoding - Input x shape:\", x.shape)\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        print(\"PositionalEncoding - Output x shape:\", x.shape)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class LayerNormalisation(nn.Module):\n",
        "    def __init__(self, epsilon=1e-6):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dff = dff\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dff)\n",
        "        self.linear2 = nn.Linear(dff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"FeedForward - Input x shape:\", x.shape)\n",
        "        x = self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
        "        print(\"FeedForward - Output x shape:\", x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "enix7wfPrHkM"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "num_heads = 8   # Number of attention heads\n",
        "dff = 256       # Feed-forward network hidden dimension\n",
        "dropout = 0.1   # Dropout rate\n",
        "d_model = 512\n",
        "h = 8\n",
        "# Instantiate the DecoderLayer\n",
        "decoder_layer = DecoderLayer(\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    dropout=dropout\n",
        ")\n"
      ],
      "metadata": {
        "id": "24pLXK_jeovC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decoder class represents the full decoder stack in a Transformer architecture. It is responsible for sequentially applying multiple DecoderLayer instances (like the one we previously discussed) to process input data, usually in tasks like machine translation, text generation, or other sequence-to-sequence tasks.\n",
        "\n",
        "num_layers: Number of DecoderLayer instances in the stack.\\\n",
        "d_model: Dimensionality of the model's embeddings and hidden states.\\\n",
        "num_heads: Number of attention heads in the multi-head attention mechanism.\\\n",
        "dff: Hidden layer size in the feed-forward network.\\\n",
        "dropout: Dropout rate for regularization.\\\n",
        "\n",
        "Components:\n",
        "\n",
        "self.layer: A ModuleList of DecoderLayer instances, each with its own attention and feed-forward blocks.\\\n",
        "self.layer_norm: A final layer normalization step applied to stabilize the output."
      ],
      "metadata": {
        "id": "lY71-EDbgaCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer = nn.ModuleList([DecoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)])\n",
        "        self.layer_norm = LayerNormalisation()\n",
        "\n",
        "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.layer[i](x, encoder_output, source_mask, target_mask)\n",
        "        return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "gw89FrbfgXB1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure d_model is divisible by num_heads\n",
        "d_model = 512  # Embedding size\n",
        "num_heads = 8  # Attention heads\n",
        "assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n"
      ],
      "metadata": {
        "id": "m2paooIklUa5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(batch_size, sequence_length, d_model)\n",
        "k = torch.randn(batch_size, sequence_length, d_model)\n",
        "v = torch.randn(batch_size, sequence_length, d_model)\n",
        "mask = None  # Example without mask\n",
        "\n",
        "# Pass through the multi-head attention block\n",
        "output = mha_block(q, k, v, mask)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # (Batch, Sequence, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQBLBrQSoFRP",
        "outputId": "fd45525b-2ae9-4fee-a41e-8c4f851e120d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the decoder\n",
        "num_layers = 6\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "dff = 256\n",
        "dropout = 0.1\n",
        "\n",
        "# Assuming Decoder is implemented as defined earlier\n",
        "decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, dropout=dropout)\n",
        "\n",
        "# Example inputs\n",
        "batch_size = 2\n",
        "target_seq_len = 10\n",
        "source_seq_len = 8\n",
        "\n",
        "x = torch.rand(batch_size, target_seq_len, d_model)  # Target sequence embeddings\n",
        "encoder_output = torch.rand(batch_size, source_seq_len, d_model)  # Encoder output\n",
        "source_mask = torch.ones(batch_size, 1, 1, source_seq_len).bool()  # Mask for encoder output\n",
        "\n",
        "# Causal mask for target sequence\n",
        "target_mask = torch.tril(torch.ones(target_seq_len, target_seq_len)).bool()  # Shape: (target_seq_len, target_seq_len)\n",
        "target_mask = target_mask.unsqueeze(0).unsqueeze(1)  # Add batch and head dimensions\n",
        "target_mask = target_mask.expand(batch_size, num_heads, -1, -1)  # Shape: (batch_size, num_heads, target_seq_len, target_seq_len)\n",
        "\n",
        "# Forward pass\n",
        "output = decoder(x, encoder_output, source_mask, target_mask)\n",
        "print(\"Decoder output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy2gxMQVne0A",
        "outputId": "60e60480-a65f-4913-a249-31dab14a3229"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "Decoder output shape: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ProjectionLayer is a neural network module designed to transform the decoder's output (or any sequence of embeddings) into a probability distribution over a vocabulary for tasks like text generation, machine translation, or language modeling."
      ],
      "metadata": {
        "id": "UMiUUlUXsiS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model, vocabulary_size):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.projection = nn.Linear(d_model, vocabulary_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (Batch, Seq_Len, D_Model) -->( Batch, Seq_Len, Vocab_Size)\n",
        "        return torch.log_softmax(self.projection(x), dim=-1)"
      ],
      "metadata": {
        "id": "bbpeBQSVsz3R"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "d_model = 512\n",
        "vocabulary_size = 10000\n",
        "\n",
        "# Instantiate the projection layer\n",
        "projection_layer = ProjectionLayer(d_model=d_model, vocabulary_size=vocabulary_size)\n",
        "\n",
        "# Example input: decoder's output embeddings\n",
        "x = torch.rand(batch_size, seq_len, d_model)  # Shape: (2, 5, 512)\n",
        "\n",
        "# Forward pass through the projection layer\n",
        "output = projection_layer(x)\n",
        "\n",
        "print(\"Projection output shape:\", output.shape)  # Should be (2, 5, 10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee-Nv5yosg5B",
        "outputId": "c53ea005-6adb-46d5-a5ab-eb67d90f16bf"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projection output shape: torch.Size([2, 5, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The output tensor contains log-probabilities over a vocabulary of size 10000 for each token in the sequence (of length 5), for each batch item (2 sequences).\n",
        "\n",
        "print(output[0, 0])  # Log-probabilities for the first token in the first sequence\n",
        "\n",
        "#This would print a tensor of shape (vocabulary_size,) (e.g., (10000,)), where each value corresponds to the log-probability of a specific word in the vocabulary.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEbDqYVBs30m",
        "outputId": "65b0392a-085a-4b5a-957c-bbdb4a9bd116"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-9.3978, -9.8450, -9.0427,  ..., -9.1781, -9.1283, -8.9257],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.dff = dff  # Feed Forward Neural Network Output Size\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = FeedForward(d_model, dff, dropout)\n",
        "        self.residual_mha = ResidualConnection(dropout)\n",
        "        self.residual_ffn = ResidualConnection(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Multi-Head Attention sub-layer\n",
        "        attn_output = self.residual_mha(x, lambda x: self.mha(x, x, x, mask))\n",
        "\n",
        "        # FeedForward sub-layer\n",
        "        ffn_output = self.residual_ffn(attn_output, self.ffn)\n",
        "\n",
        "        return ffn_output\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer = nn.ModuleList([EncoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)])\n",
        "        self.layer_norm = LayerNormalisation()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.layer[i](x, mask)\n",
        "        return self.layer_norm(x)\n"
      ],
      "metadata": {
        "id": "4fn6xb-0m13I"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout, source_embeddings, target_embeddings,\n",
        "                 source_pos_encodings, target_pos_encodings, vocabulary_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, dropout)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, dropout)\n",
        "        self.projection = ProjectionLayer(d_model, vocabulary_size)\n",
        "        self.source_embeddings = source_embeddings\n",
        "        self.target_embeddings = target_embeddings\n",
        "        self.source_pos_encodings = source_pos_encodings\n",
        "        self.target_pos_encodings = target_pos_encodings\n",
        "\n",
        "    def encode(self, source_input, source_mask):\n",
        "        # Embedding and positional encoding for source inputs\n",
        "        source_embedded = self.source_embeddings(source_input)\n",
        "        source_embedded = self.source_pos_encodings(source_embedded)\n",
        "        # Pass source input through the encoder\n",
        "        encoder_output = self.encoder(source_embedded, source_mask)\n",
        "        return encoder_output\n",
        "\n",
        "    def decode(self, target_input, encoder_output, source_mask, target_mask):\n",
        "        # Embedding and positional encoding for target inputs\n",
        "        target_embedded = self.target_embeddings(target_input)\n",
        "        target_embedded = self.target_pos_encodings(target_embedded)\n",
        "        # Pass target input through the decoder\n",
        "        decoder_output = self.decoder(target_embedded, encoder_output, source_mask, target_mask)\n",
        "        return decoder_output\n",
        "\n",
        "    def project(self, decoder_output):\n",
        "        # Project the decoder output to the vocabulary size\n",
        "        output_logits = self.projection(decoder_output)\n",
        "        return output_logits\n"
      ],
      "metadata": {
        "id": "4EwqAYnTtND2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods:\n",
        "\n",
        "encode\n",
        "Input:\n",
        "source_input: Source sequence tokens (shape: (batch_size, source_seq_len)).\n",
        "source_mask: Attention mask for the source sequence.\n",
        "Process:\n",
        "Applies embeddings and positional encodings to source_input.\n",
        "Feeds the resulting sequence into the encoder.\n",
        "Output:\n",
        "encoder_output: Contextualized representations of the source sequence (shape: (batch_size, source_seq_len, d_model)).\n",
        "decode\n",
        "Input:\n",
        "target_input: Target sequence tokens (shape: (batch_size, target_seq_len)).\n",
        "encoder_output: Output from the encoder.\n",
        "source_mask: Attention mask for the source sequence.\n",
        "target_mask: Causal mask for the target sequence.\n",
        "Process:\n",
        "Applies embeddings and positional encodings to target_input.\n",
        "Feeds the resulting sequence, along with encoder_output, into the decoder.\n",
        "Output:\n",
        "decoder_output: Representations of the target sequence (shape: (batch_size, target_seq_len, d_model)).\n",
        "project\n",
        "Input:\n",
        "decoder_output: Output of the decoder.\n",
        "Process:\n",
        "Passes decoder_output through the ProjectionLayer, projecting it to the vocabulary space.\n",
        "Output:\n",
        "output_logits: Log-probabilities over the vocabulary for each token in the target sequence (shape: (batch_size, target_seq_len, vocabulary_size)).\n",
        "What Does This Do?\n",
        "The Transformer class performs end-to-end sequence processing for tasks like translation or text generation. Here's how it works in practice:\n",
        "\n",
        "Encoding:\n",
        "\n",
        "Converts the source sequence into contextual embeddings using the encoder.\n",
        "Encodings capture both token-level and sequence-level dependencies.\n",
        "Decoding:\n",
        "\n",
        "Processes the target sequence (partially decoded tokens).\n",
        "Attends to the encoder output to incorporate information from the source sequence.\n",
        "Projection:\n",
        "\n",
        "Maps decoder outputs to the target vocabulary, producing probabilities for the next tokens."
      ],
      "metadata": {
        "id": "UcpVjrcrmECp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example parameters\n",
        "num_layers = 6\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "dff = 2048\n",
        "dropout = 0.1\n",
        "vocabulary_size = 10000\n",
        "source_seq_len = 10\n",
        "target_seq_len = 8\n",
        "batch_size = 2\n",
        "\n",
        "# Dummy embeddings and positional encodings (replace with learned embeddings in practice)\n",
        "source_embeddings = nn.Embedding(5000, d_model)\n",
        "target_embeddings = nn.Embedding(10000, d_model)\n",
        "source_pos_encodings = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU())  # Dummy positional encoding\n",
        "target_pos_encodings = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU())\n",
        "\n",
        "# Instantiate the Transformer\n",
        "transformer = Transformer(\n",
        "    num_layers, d_model, num_heads, dff, dropout,\n",
        "    source_embeddings, target_embeddings,\n",
        "    source_pos_encodings, target_pos_encodings,\n",
        "    vocabulary_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "uCClkyC8SK7r"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inputs\n",
        "source_input = torch.randint(0, 5000, (batch_size, source_seq_len))  # Source tokens\n",
        "target_input = torch.randint(0, 10000, (batch_size, target_seq_len))  # Target tokens\n",
        "\n",
        "# Masks\n",
        "source_mask = torch.ones(batch_size, 1, 1, source_seq_len).bool()  # Source mask\n",
        "target_mask = torch.tril(torch.ones(target_seq_len, target_seq_len)).unsqueeze(0).unsqueeze(0).bool()  # Causal mask\n",
        "\n",
        "# Forward pass\n",
        "encoder_output = transformer.encode(source_input, source_mask)\n",
        "decoder_output = transformer.decode(target_input, encoder_output, source_mask, target_mask)\n",
        "output_logits = transformer.project(decoder_output)\n",
        "\n",
        "print(\"Logits shape:\", output_logits.shape)  # Should be (batch_size, target_seq_len, vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hHQtShvmQBl",
        "outputId": "44369cd3-fc17-464e-f3fe-b0d61135c09a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 8, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 8, 512])\n",
            "Logits shape: torch.Size([2, 8, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbF-l42Inp0-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}