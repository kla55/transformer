{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kla55/transformer/blob/main/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PcQkM8AMZU3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FtGJMT3CZEg3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int): # constructor - needs dimensions and vocab size\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model) # mapping between numbers and vector size - 512\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model) # sqrt"
      ],
      "metadata": {
        "id": "rJ5-04bLZaVg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "d_model = 512\n",
        "vocab_size = 10000\n",
        "input_embeddings = InputEmbeddings(d_model=d_model, vocab_size=vocab_size)\n",
        "\n",
        "# Sample input: batch of sequences with token IDs\n",
        "sample_input = torch.randint(0, vocab_size, (4, 10))  # Batch of 4 sequences, each with 10 tokens\n",
        "embedded_output = input_embeddings(sample_input)\n",
        "\n",
        "print(\"Input shape:\", sample_input.shape)  # (batch_size, sequence_length)\n",
        "print(\"Output shape:\", embedded_output.shape)  # (batch_size, sequence_length, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLeAeUIDcoCj",
        "outputId": "8e3b1900-b44b-4829-9f43-a6d2749cc17c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 10])\n",
            "Output shape: torch.Size([4, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # create an array\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        print(pe.shape)\n",
        "        # create a position tensor\n",
        "        # - Adds an additional dimension to the tensor at index 1, converting the 1D tensor into a 2D tensor - (seq_len, 1)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        print(position.shape)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        print(div_term.shape)\n",
        "        # apply the sin to even positions and cos to odd positions\n",
        "        # pe[all vocab, starting at position 0/1 and for every 2]\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0) # (seq_len, d_model) -> (1, seq_len, d_model)\n",
        "        # A buffer is a persistent tensor in the model that is not considered a learnable parameter (i.e., it won't be updated during backpropagation).\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # to add this positional encoding to every word inside the sentence\n",
        "        # extracts the part of the positional encoding needed for the current input and locks it so it won't change during training\n",
        "        # :x.size(1): Selects elements up to the length of the sequence dimension of x. This means that the operation is selecting a subset of self.pe that matches the sequence length of the input tensor x.\n",
        "        x = x + (self.pe[:, :x.size(1), :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "IGmmxMf2at5Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "d_model = 512\n",
        "seq_len = 10\n",
        "dropout = 0.1\n",
        "pos_encoding = PositionalEncoding(d_model=d_model, seq_len=seq_len, dropout=dropout)\n",
        "\n",
        "# Sample input: batch of embeddings\n",
        "batch_size = 4\n",
        "sample_embeddings = torch.randn(batch_size, seq_len, d_model)  # Random embeddings for a batch of 4 sequences\n",
        "\n",
        "# Apply positional encoding\n",
        "encoded_output = pos_encoding(sample_embeddings)\n",
        "\n",
        "print(\"Input shape:\", sample_embeddings.shape)  # (batch_size, seq_len, d_model)\n",
        "print(\"Output shape:\", encoded_output.shape)     # (batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNDvThqTfKO6",
        "outputId": "99552b0d-0374-4de7-d5ff-1148dafa122b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 512])\n",
            "torch.Size([10, 1])\n",
            "torch.Size([256])\n",
            "Input shape: torch.Size([4, 10, 512])\n",
            "Output shape: torch.Size([4, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  \"\"\"\n",
        "  Layer normalization is a technique used in neural networks to stabilize and accelerate the training process.\n",
        "  It normalizes the inputs across the features of each layer, which helps in making the model more robust and easier to train.\n",
        "\n",
        "  In layer normalization, the mean and variance are computed for each individual sample across all the features (or neurons) within a layer.\n",
        "  The input to a particular layer is normalized by subtracting the mean and dividing by the standard deviation calculated over the features of that input. This results in inputs that have a mean of 0 and a standard deviation of 1.\n",
        "  After normalization, the output is typically scaled and shifted using learnable parameters (gamma and beta) so that the network can still represent a wide range of inputs if needed.\n",
        "  Need Epislon for stability - if sigma is close to 0 then the mew value becomes big - so we do not want big or small values\n",
        "  \"\"\"\n",
        "  def __init__(self, eps: float = 10**6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.alpha = nn.Parameter(torch.ones(1)) # multiplier\n",
        "    self.beta = nn.Parameter(torch.zeros(1)) # additive\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.alpha * (x - mean) / (std + self.eps)"
      ],
      "metadata": {
        "id": "bSpVIuoXXHfE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalisation(nn.Module):\n",
        "    def __init__(self, epsilon=1e-6):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias"
      ],
      "metadata": {
        "id": "kI00s5EOl_fT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "layer_norm = LayerNormalization(eps=1e-6)\n",
        "\n",
        "# Create a batch of input data\n",
        "input_data = torch.randn(4, 6)  # Batch of 4 samples, each with 6 features\n",
        "\n",
        "# Apply layer normalization\n",
        "normalized_output = layer_norm(input_data)\n",
        "\n",
        "print(\"Input data:\\n\", input_data)\n",
        "print(\"\\nNormalized output:\\n\", normalized_output)\n",
        "print(\"\\nOutput mean (per sample):\", normalized_output.mean(-1))  # Should be close to 0\n",
        "print(\"Output std (per sample):\", normalized_output.std(-1))    # Should be close to 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_zIZN40jj71",
        "outputId": "03bef504-7a71-47df-cc74-a9ba779eab3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:\n",
            " tensor([[ 0.2011, -2.4617,  0.8124, -0.1935, -0.6042,  1.1987],\n",
            "        [-0.5959,  0.7405, -0.2917,  0.5042,  0.4152, -0.0412],\n",
            "        [ 0.6398, -0.3841, -0.0936,  0.0431,  0.2456,  0.7659],\n",
            "        [-1.9061,  0.8484,  0.6571, -2.1188, -1.6789, -1.0248]])\n",
            "\n",
            "Normalized output:\n",
            " tensor([[ 0.2895, -1.7628,  0.7607, -0.0146, -0.3311,  1.0584],\n",
            "        [-1.3928,  1.2005, -0.8025,  0.7420,  0.5692, -0.3164],\n",
            "        [ 0.9934, -1.3340, -0.6738, -0.3630,  0.0974,  1.2800],\n",
            "        [-0.7898,  1.3109,  1.1651, -0.9520, -0.6165, -0.1177]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "\n",
            "Output mean (per sample): tensor([-1.1021e-08,  0.0000e+00,  9.9341e-09,  1.9868e-08],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "Output std (per sample): tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (Batch, Sequence, d_model) -> (Batch, Sequence, dff) -> (Batch, Sequence, d_model)\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "id": "XUkrHMoMbZcE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "ff_block = FeedForwardBlock(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "\n",
        "# Input data: batch of 4 sequences, each with 10 tokens and an embedding dimension of 512\n",
        "input_data = torch.randn(4, 10, d_model)\n",
        "\n",
        "# Pass through the feedforward block\n",
        "output_data = ff_block(input_data)\n",
        "\n",
        "print(\"Input shape:\", input_data.shape)  # (Batch, Sequence, d_model)\n",
        "print(\"Output shape:\", output_data.shape)  # Should also be (Batch, Sequence, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E5F3rvV1_QL",
        "outputId": "60550344-f8d7-49e7-8f34-25d4774d2bf4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 10, 512])\n",
            "Output shape: torch.Size([4, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.attention_scores = None\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.d_k = d_model // self.num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.layer_norm1 = LayerNormalisation()\n",
        "        self.layer_norm2 = LayerNormalisation()\n",
        "        self.layer_norm3 = LayerNormalisation()\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(q, k, v, mask, dropout):\n",
        "        d_k = q.shape[-1]\n",
        "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1)  # (Batch, num_heads, Seq_Len,  Seq_Len)\n",
        "\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        attn = torch.matmul(attention_scores, v)  # (Batch, num_heads, Seq_Len, d_k)\n",
        "        return attn, attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "\n",
        "        q = self.wq(q)\n",
        "\n",
        "        k = self.wk(k)\n",
        "\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = q.view(q.shape[0], q.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        k = k.view(k.shape[0], k.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        v = v.view(v.shape[0], v.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, self.attention_scores = MultiHeadAttention.attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "        x = self.wo(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "29gHj0ocq9ZC"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, h: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // h\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "    d_k = query.shape[-1]\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(torch.softmax(attention_scores, dim=-1))\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    # q = [batch size, query len, hid dim]\n",
        "    # k = [batch size, key len, hid dim]\n",
        "    # v = [batch size, value len, hid dim]\n",
        "    query = self.w_q(q)\n",
        "    key = self.w_k(k)\n",
        "    value = self.w_v(v)\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) #.permute(0, 2, 1, 3)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "    x = self.w_o(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "grdHQVW7jyaV"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "h = 8\n",
        "dropout = 0.1\n",
        "mha_block = MultiHeadAttentionBlock(d_model=d_model, h=h, dropout=dropout)\n",
        "\n",
        "# Define input tensors for a batch of sequences\n",
        "batch_size = 4\n",
        "sequence_length = 10\n",
        "q = torch.randn(batch_size, sequence_length, d_model)\n",
        "k = torch.randn(batch_size, sequence_length, d_model)\n",
        "v = torch.randn(batch_size, sequence_length, d_model)\n",
        "mask = None  # Example without mask\n",
        "\n",
        "# Pass through the multi-head attention block\n",
        "output = mha_block(q, k, v, mask)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # (Batch, Sequence, d_model)"
      ],
      "metadata": {
        "id": "wlCkhu3u3uWV",
        "outputId": "21163227-2cc4-4fe9-8d15-85851c8852e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([4, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, dropout: float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.norm(x))) # residual connection"
      ],
      "metadata": {
        "id": "A_NNizN6x-8M"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))"
      ],
      "metadata": {
        "id": "v3p_oykaUeDC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input dimensions\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "d_model = 10\n",
        "d_ff = 20\n",
        "\n",
        "# Instantiate modules\n",
        "dropout_rate = 0.1\n",
        "residual_connection = ResidualConnection(dropout=dropout_rate)\n",
        "feedforward = FeedForwardLayer(d_model=d_model, d_ff=d_ff)\n",
        "# Create a sample input\n",
        "x = torch.rand(batch_size, seq_len, d_model)\n",
        "# Apply ResidualConnection with the FeedForwardLayer as the sublayer\n",
        "output = residual_connection(x, feedforward)\n",
        "\n",
        "print(\"Input Shape:\", x.shape)\n",
        "print(\"Output Shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytHNoGGYTycg",
        "outputId": "b3733201-2167-443d-949d-1f05cb550e4f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([2, 5, 10])\n",
            "Output Shape: torch.Size([2, 5, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: Feedforwardblock, dropout: float):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "kns6E0Mzy-nE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input parameters\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "d_ff = 256\n",
        "dropout = 0.1\n",
        "\n",
        "# Instantiate components\n",
        "self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
        "feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "encoder_block = EncoderBlock(self_attention_block, feed_forward_block, dropout)\n",
        "\n",
        "# Example input\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "x = torch.rand(seq_len, batch_size, d_model)  # Transformer input is (seq_len, batch_size, d_model)\n",
        "src_mask = None  # Example without masking\n",
        "\n",
        "# Forward pass\n",
        "output = encoder_block(x, src_mask)\n",
        "print(\"Input Shape:\", x.shape)\n",
        "print(\"Output Shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilx3yIhtBM1E",
        "outputId": "0de27d5b-1d9f-40dd-c8b7-0ccf8a981bbf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([10, 2, 64])\n",
            "Output Shape: torch.Size([10, 2, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "pdAz2SsC4Lgq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36B2s1qonxUY"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example configuration\n",
        "features = 64\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "d_ff = 256\n",
        "self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
        "feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "\n",
        "# Create multiple EncoderBlocks\n",
        "encoder_layers = nn.ModuleList([\n",
        "    EncoderBlock(self_attention_block, feed_forward_block, dropout) for _ in range(num_layers)\n",
        "])\n",
        "\n",
        "# Instantiate the Encoder\n",
        "encoder = Encoder(layers=encoder_layers)\n",
        "\n",
        "# Input tensor (sequence length, batch size, feature size)\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "x = torch.rand(seq_len, batch_size, features)\n",
        "mask = None  # Example without masking\n",
        "\n",
        "# Forward pass\n",
        "output = encoder(x, mask)\n",
        "\n",
        "print(\"Input Shape:\", x.shape)\n",
        "print(\"Output Shape:\", output.shape)"
      ],
      "metadata": {
        "id": "j0Avhi4g4WYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ee941e-209d-467f-8c76-ab706c3bd4a6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([10, 2, 64])\n",
            "Output Shape: torch.Size([10, 2, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cEf8wqpiREs2"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff  # Feed Forward Neural Network Output Size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = FeedForward(d_model, dff, dropout)\n",
        "        self.residual_mha = ResidualConnection(dropout)\n",
        "        self.residual_cross_mha = ResidualConnection(dropout)\n",
        "        self.residual_ffn = ResidualConnection(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
        "        # Multi-Head Attention sub-layer\n",
        "        attn_output = self.residual_mha(x, lambda x: self.mha(x, x, x, target_mask))\n",
        "\n",
        "        # Cross-Attention sub-layer\n",
        "        cross_attn_output = self.residual_cross_mha(attn_output,\n",
        "                                                    lambda x: self.mha(x, encoder_output, encoder_output, source_mask))\n",
        "\n",
        "        # FeedForward sub-layer\n",
        "        ffn_output = self.residual_ffn(cross_attn_output, self.ffn)\n",
        "\n",
        "        return ffn_output"
      ],
      "metadata": {
        "id": "i-Ask2r2er69"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"InputEmbeddings - Input x shape:\", x.shape)\n",
        "        embeddings = self.word_embeddings(x) * math.sqrt(self.d_model)\n",
        "        print(\"InputEmbeddings - Output embeddings shape:\", embeddings.shape)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"PositionalEncoding - Input x shape:\", x.shape)\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        print(\"PositionalEncoding - Output x shape:\", x.shape)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class LayerNormalisation(nn.Module):\n",
        "    def __init__(self, epsilon=1e-6):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dff = dff\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dff)\n",
        "        self.linear2 = nn.Linear(dff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"FeedForward - Input x shape:\", x.shape)\n",
        "        x = self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
        "        print(\"FeedForward - Output x shape:\", x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "enix7wfPrHkM"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "num_heads = 8   # Number of attention heads\n",
        "dff = 256       # Feed-forward network hidden dimension\n",
        "dropout = 0.1   # Dropout rate\n",
        "d_model = 512\n",
        "h = 8\n",
        "# Instantiate the DecoderLayer\n",
        "decoder_layer = DecoderLayer(\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    dropout=dropout\n",
        ")\n"
      ],
      "metadata": {
        "id": "24pLXK_jeovC"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decoder class represents the full decoder stack in a Transformer architecture. It is responsible for sequentially applying multiple DecoderLayer instances (like the one we previously discussed) to process input data, usually in tasks like machine translation, text generation, or other sequence-to-sequence tasks.\n",
        "\n",
        "num_layers: Number of DecoderLayer instances in the stack.\\\n",
        "d_model: Dimensionality of the model's embeddings and hidden states.\\\n",
        "num_heads: Number of attention heads in the multi-head attention mechanism.\\\n",
        "dff: Hidden layer size in the feed-forward network.\\\n",
        "dropout: Dropout rate for regularization.\\\n",
        "\n",
        "Components:\n",
        "\n",
        "self.layer: A ModuleList of DecoderLayer instances, each with its own attention and feed-forward blocks.\\\n",
        "self.layer_norm: A final layer normalization step applied to stabilize the output."
      ],
      "metadata": {
        "id": "lY71-EDbgaCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer = nn.ModuleList([DecoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)])\n",
        "        self.layer_norm = LayerNormalisation()\n",
        "\n",
        "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.layer[i](x, encoder_output, source_mask, target_mask)\n",
        "        return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "gw89FrbfgXB1"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure d_model is divisible by num_heads\n",
        "d_model = 512  # Embedding size\n",
        "num_heads = 8  # Attention heads\n",
        "assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n"
      ],
      "metadata": {
        "id": "m2paooIklUa5"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(batch_size, sequence_length, d_model)\n",
        "k = torch.randn(batch_size, sequence_length, d_model)\n",
        "v = torch.randn(batch_size, sequence_length, d_model)\n",
        "mask = None  # Example without mask\n",
        "\n",
        "# Pass through the multi-head attention block\n",
        "output = mha_block(q, k, v, mask)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # (Batch, Sequence, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQBLBrQSoFRP",
        "outputId": "398fbf17-1e41-43d3-804a-961e85bbc183"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the decoder\n",
        "num_layers = 6\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "dff = 256\n",
        "dropout = 0.1\n",
        "\n",
        "# Assuming Decoder is implemented as defined earlier\n",
        "decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, dropout=dropout)\n",
        "\n",
        "# Example inputs\n",
        "batch_size = 2\n",
        "target_seq_len = 10\n",
        "source_seq_len = 8\n",
        "\n",
        "x = torch.rand(batch_size, target_seq_len, d_model)  # Target sequence embeddings\n",
        "encoder_output = torch.rand(batch_size, source_seq_len, d_model)  # Encoder output\n",
        "source_mask = torch.ones(batch_size, 1, 1, source_seq_len).bool()  # Mask for encoder output\n",
        "\n",
        "# Causal mask for target sequence\n",
        "target_mask = torch.tril(torch.ones(target_seq_len, target_seq_len)).bool()  # Shape: (target_seq_len, target_seq_len)\n",
        "target_mask = target_mask.unsqueeze(0).unsqueeze(1)  # Add batch and head dimensions\n",
        "target_mask = target_mask.expand(batch_size, num_heads, -1, -1)  # Shape: (batch_size, num_heads, target_seq_len, target_seq_len)\n",
        "\n",
        "# Forward pass\n",
        "output = decoder(x, encoder_output, source_mask, target_mask)\n",
        "print(\"Decoder output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy2gxMQVne0A",
        "outputId": "4f3894a3-8c4f-4e38-f8e8-15f2bb1c91e4"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Input x shape: torch.Size([2, 10, 512])\n",
            "FeedForward - Output x shape: torch.Size([2, 10, 512])\n",
            "Decoder output shape: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    }
  ]
}