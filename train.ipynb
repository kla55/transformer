{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEGfseHq2lcnZ4Uq4yYphC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kla55/transformer/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH-Yl-DNd3Ft",
        "outputId": "0b935713-6f28-4a5a-d923-3670c098cb14"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ8e1Yl0eIuP",
        "outputId": "13178dd5-37f6-4718-9004-099725018ad4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dataset in /usr/local/lib/python3.10/dist-packages (1.6.2)\n",
            "Requirement already satisfied: sqlalchemy<2.0.0,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.4.54)\n",
            "Requirement already satisfied: alembic>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.14.0)\n",
            "Requirement already satisfied: banal>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.0.6)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (1.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=0.6.2->dataset) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        'lang_source': 'en',\n",
        "        'lang_target': 'it',\n",
        "        'tokenizer_file': 'tokenizer_{0}.json',  # Provide the path to your tokenizer directory\n",
        "        'batch_size': 1,\n",
        "        'num_layers': 4,\n",
        "        'd_model': 512,\n",
        "        'num_heads': 8,\n",
        "        'dff': 1024,\n",
        "        'dropout': 0.1,\n",
        "        'learning_rate': 10 ** -4,\n",
        "        'num_epochs': 20,\n",
        "        'model_folder': \"weights\",\n",
        "        'model_basename': \"transformer_model_\",\n",
        "        'preload': None,\n",
        "        'experiment_name': \"runs/transformer_model\"\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "def get_weights_file_path(config, epoch):\n",
        "    model_folder = config['model_folder']\n",
        "    model_basename = config['model_basename']\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)"
      ],
      "metadata": {
        "id": "NhVGU0ktwP76"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer_source, tokenizer_target, source_lang, target_lang, seq_len):\n",
        "        \"\"\"\n",
        "        Bilingual dataset class for training a sequence-to-sequence model.\n",
        "\n",
        "        Args:\n",
        "            dataset (list): List of dictionaries containing source and target translations.\n",
        "            tokenizer_source: Tokenizer for source language.\n",
        "            tokenizer_target: Tokenizer for target language.\n",
        "            source_lang (str): Key for accessing source language in the dataset dictionary.\n",
        "            target_lang (str): Key for accessing target language in the dataset dictionary.\n",
        "            seq_len (int): Maximum sequence length for encoder and decoder inputs.\n",
        "        \"\"\"\n",
        "        self.seq_len = seq_len\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer_source = tokenizer_source\n",
        "        self.tokenizer_target = tokenizer_target\n",
        "        self.source_lang = source_lang\n",
        "        self.target_lang = target_lang\n",
        "        self.sos_token = torch.tensor([tokenizer_source.token_to_id('[SOS]')], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_source.token_to_id('[EOS]')], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_source.token_to_id('[PAD]')], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves a sample from the dataset and preprocesses it.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            encoder_input (Tensor): Padded encoder input sequence.\n",
        "            decoder_input (Tensor): Padded decoder input sequence.\n",
        "            masks, labels, and text data.\n",
        "        \"\"\"\n",
        "        source_target_pair = self.dataset[index]\n",
        "        source_text = source_target_pair['translation'][self.source_lang]\n",
        "        target_text = source_target_pair['translation'][self.target_lang]\n",
        "\n",
        "        encoder_input_tokens = self.tokenizer_source.encode(source_text).ids\n",
        "        decoder_input_tokens = self.tokenizer_target.encode(target_text).ids\n",
        "\n",
        "        encoder_num_padding_tokens = self.seq_len - len(encoder_input_tokens) - 2\n",
        "        decoder_num_padding_tokens = self.seq_len - len(decoder_input_tokens) - 1\n",
        "\n",
        "        if encoder_num_padding_tokens < 0 or decoder_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(encoder_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * encoder_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(decoder_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * decoder_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(decoder_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * decoder_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            'encoder_input': encoder_input,  # (seq_len)\n",
        "            'decoder_input': decoder_input,  # (seq_len)\n",
        "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # (1, 1, seq_len)\n",
        "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
        "            # (1, seq_len) & (1, seq_len, seq_len),\n",
        "            'target_label': label,  # (seq_len)\n",
        "            'source_text': source_text,\n",
        "            'target_text': target_text,\n",
        "        }\n",
        "\n",
        "\n",
        "def causal_mask(size):\n",
        "    \"\"\"\n",
        "        Creates a causal mask for the decoder.\n",
        "\n",
        "        Args:\n",
        "            size (int): Size of the mask.\n",
        "\n",
        "        Returns:\n",
        "            causal_mask (Tensor): Causal mask.\n",
        "        \"\"\"\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0"
      ],
      "metadata": {
        "id": "hYZpiO42vf8-"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Eh2caaard2MM"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from config import get_weights_file_path, get_config\n",
        "# from dataset import BilingualDataset, causal_mask\n",
        "# from model import build_transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tokenizer(config, dataset, lang):\n",
        "    # eg config['tokenizer_file'] = '../tokenizer/tokenizer_en.json'\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    print(tokenizer_path)\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(dataset, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def get_all_sentences(dataset, lang):\n",
        "  for sentence in dataset:\n",
        "      yield sentence['translation'][lang]"
      ],
      "metadata": {
        "id": "sXAQBpXQ4qNm"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = get_config()"
      ],
      "metadata": {
        "id": "ov5LrvkeAhYF"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('opus_books', f'{config[\"lang_source\"]}-{config[\"lang_target\"]}', split='train')"
      ],
      "metadata": {
        "id": "dTPIfJBx7IVX"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcfeHgh0A0HK",
        "outputId": "4977b74c-8d37-484c-b6d5-a809704a033a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'translation'],\n",
              "    num_rows: 32332\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_source = build_tokenizer(config, dataset, config['lang_source'])\n",
        "tokenizer_source"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VB_T18QA6oG",
        "outputId": "f251e654-75f6-40a0-ad5d-f3af56b771e2"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_en.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"[UNK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"[SOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":3, \"content\":\"[EOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=WordLevel(vocab={\"[UNK]\":0, \"[PAD]\":1, \"[SOS]\":2, \"[EOS]\":3, \",\":4, \"the\":5, \"and\":6, \".\":7, \"to\":8, \"I\":9, \"of\":10, \"a\":11, \"'\":12, \"in\":13, \"was\":14, \"that\":15, \"he\":16, \"it\":17, \";\":18, \"had\":19, \"his\":20, \"not\":21, \"with\":22, \"her\":23, \"you\":24, \"as\":25, \"for\":26, \"she\":27, \"my\":28, \"-\":29, \"at\":30, \"but\":31, \"him\":32, \"me\":33, \"is\":34, \"\"\":35, \"on\":36, \"be\":37, \":\":38, \"said\":39, \"have\":40, \"s\":41, \"all\":42, \"which\":43, \"so\":44, \"they\":45, \"by\":46, \"one\":47, \"were\":48, \"this\":49, \"them\":50, \"would\":51, \"from\":52, \",'\":53, \"or\":54, \"what\":55, \"up\":56, \"could\":57, \"!\":58, \"when\":59, \"He\":60, \"out\":61, \"The\":62, \"been\":63, \"there\":64, \"an\":65, \"are\":66, \"no\":67, \"who\":68, \"if\":69, \"we\":70, \"now\":71, \"about\":72, \"did\":73, \".\"\":74, \"very\":75, \".'\":76, \"will\":77, \"--\":78, \"their\":79, \"?\":80, \"do\":81, \"more\":82, \"t\":83, \"?'\":84, \"!'\":85, \"Levin\":86, \"But\":87, \"It\":88, \"into\":89, \"only\":90, \"And\":91, \"then\":92, \"some\":93, \"time\":94, \"your\":95, \"like\":96, \"go\":97, \"thought\":98, ...}, unk_token=\"[UNK]\"))"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_target = build_tokenizer(config, dataset, config['lang_target'])\n",
        "tokenizer_target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ngFgmt4Cp0F",
        "outputId": "21ac9281-57c4-4225-b5e9-3d6987764ad3"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_it.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"[UNK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"[SOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":3, \"content\":\"[EOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=WordLevel(vocab={\"[UNK]\":0, \"[PAD]\":1, \"[SOS]\":2, \"[EOS]\":3, \",\":4, \".\":5, \"e\":6, \"di\":7, \"che\":8, \"—\":9, \"’\":10, \"la\":11, \"non\":12, \"a\":13, \"il\":14, \"un\":15, \"in\":16, \"per\":17, \"si\":18, \";\":19, \"con\":20, \"una\":21, \"era\":22, \"le\":23, \"l\":24, \"mi\":25, \"ma\":26, \"è\":27, \"da\":28, \"'\":29, \"?\":30, \"del\":31, \"i\":32, \"come\":33, \"più\":34, \"della\":35, \"lo\":36, \"disse\":37, \"gli\":38, \"al\":39, \"aveva\":40, \"!\":41, \"se\":42, \":\":43, \"io\":44, \"alla\":45, \"d\":46, \"E\":47, \"lui\":48, \"questo\":49, \"sua\":50, \"me\":51, \"Ma\":52, \"tutto\":53, \"Non\":54, \"così\":55, \"nel\":56, \"mia\":57, \"egli\":58, \"Levin\":59, \"o\":60, \"cosa\":61, \"suo\":62, \"ne\":63, \"vi\":64, \"mio\":65, \"quando\":66, \"lei\":67, \"nella\":68, \"loro\":69, \"perché\":70, \"sono\":71, \"quella\":72, \"ad\":73, \"senza\":74, \"ci\":75, \"quel\":76, \"quello\":77, \"ed\":78, \"La\":79, \"delle\":80, \"Il\":81, \"su\":82, \"due\":83, \"mai\":84, \"prima\":85, \"erano\":86, \"ho\":87, \"ora\":88, \"tutti\":89, \"essere\":90, \"tempo\":91, \"uno\":92, \"dei\":93, \"stato\":94, \"sempre\":95, \"dopo\":96, \"dalla\":97, \"dal\":98, ...}, unk_token=\"[UNK]\"))"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset_size = int(len(dataset) * 0.9)\n",
        "validation_dataset_size = len(dataset) - training_dataset_size\n",
        "training_dataset_raw, validation_dataset_raw = torch.utils.data.random_split(dataset, [training_dataset_size,\n",
        "                                                                                        validation_dataset_size])\n",
        "print(len(training_dataset_raw), len(validation_dataset_raw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9hC1-517Q6D",
        "outputId": "24117669-25f1-4c5f-b5bd-3983a6f4a868"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29098 3234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_source = 0\n",
        "max_len_target = 0\n",
        "for item in training_dataset_raw:\n",
        "    source_text = item['translation'][config['lang_source']]\n",
        "    target_text = item['translation'][config['lang_target']]\n",
        "    max_len_source = max(max_len_source, len(tokenizer_source.encode(source_text).ids))\n",
        "    max_len_target = max(max_len_target, len(tokenizer_target.encode(target_text).ids))"
      ],
      "metadata": {
        "id": "cihoJhNI72wj"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = BilingualDataset(training_dataset_raw, tokenizer_source, tokenizer_target,\n",
        "                                    config['lang_source'],\n",
        "                                    config['lang_target'], max_len_target)"
      ],
      "metadata": {
        "id": "wYHOMhmu_Rlm"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(config):\n",
        "    dataset = load_dataset('opus_books', f'{config[\"lang_source\"]}-{config[\"lang_target\"]}', split='train')\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_source = build_tokenizer(config, dataset, config['lang_source'])\n",
        "    tokenizer_target = build_tokenizer(config, dataset, config['lang_target'])\n",
        "\n",
        "    # Keep 90% for training and 10% for validation\n",
        "    training_dataset_size = int(len(dataset) * 0.9)\n",
        "    validation_dataset_size = len(dataset) - training_dataset_size\n",
        "    training_dataset_raw, validation_dataset_raw = torch.utils.data.random_split(dataset, [training_dataset_size,\n",
        "                                                                                           validation_dataset_size])\n",
        "\n",
        "    # # Calculate the maximum sequence lengths for source and target languages\n",
        "    '''The goal is to determine the longest sequence of tokens (in terms of tokenized IDs) in both the source and target languages within the training dataset. These maximum lengths are used later to define the maximum sequence lengths for padding or truncation during training.\n",
        "    '''\n",
        "    max_len_source = 0\n",
        "    max_len_target = 0\n",
        "    for item in training_dataset_raw:\n",
        "        source_text = item['translation'][config['lang_source']]\n",
        "        target_text = item['translation'][config['lang_target']]\n",
        "        max_len_source = max(max_len_source, len(tokenizer_source.encode(source_text).ids))\n",
        "        max_len_target = max(max_len_target, len(tokenizer_target.encode(target_text).ids))\n",
        "\n",
        "    training_dataset = BilingualDataset(training_dataset_raw, tokenizer_source, tokenizer_target,\n",
        "                                        config['lang_source'],\n",
        "                                        config['lang_target'], max_len_target)\n",
        "\n",
        "    validation_dataset = BilingualDataset(validation_dataset_raw, tokenizer_source, tokenizer_target,\n",
        "                                          config['lang_source'],\n",
        "                                          config['lang_target'], max_len_target)\n",
        "\n",
        "    # Set the maximum sequence lengths in the configuration\n",
        "    config['seq_len'] = max_len_source\n",
        "    config['max_seq_len'] = max_len_source\n",
        "\n",
        "    print(f\" Max length of source text: {max_len_source}\")\n",
        "    print(f\" Max length of target text: {max_len_target}\")\n",
        "\n",
        "    # Create data loaders for training and validation datasets\n",
        "    training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    return training_dataloader, validation_dataloader, tokenizer_source, tokenizer_target\n",
        "\n"
      ],
      "metadata": {
        "id": "yiZFjVWX4hdc"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataloader, validation_dataloader, tokenizer_source, tokenizer_target = get_dataset(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plQ9SLI2_gl6",
        "outputId": "8e693160-5b97-441e-b083-15e740c45f93"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_en.json\n",
            "tokenizer_it.json\n",
            " Max length of source text: 309\n",
            " Max length of target text: 274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Model"
      ],
      "metadata": {
        "id": "lfe1Lqv341fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config, vocab_source_length, vocab_target_length):\n",
        "    \"\"\"\n",
        "    Builds and returns a transformer model.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Configuration settings.\n",
        "        vocab_source_length (int): Vocabulary size for source language.\n",
        "        vocab_target_length (int): Vocabulary size for target language.\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): Transformer model.\n",
        "    \"\"\"\n",
        "    # Extract model configuration parameters from config\n",
        "    num_layers = config['num_layers']\n",
        "    d_model = config['d_model']\n",
        "    num_heads = config['num_heads']\n",
        "    dff = config['dff']\n",
        "    dropout = config['dropout']\n",
        "    max_seq_len = config['seq_len']\n",
        "\n",
        "    # Build the transformer model\n",
        "    model = build_transformer(num_layers, d_model, num_heads, dff, dropout, vocab_source_length,\n",
        "                              vocab_target_length, max_seq_len)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "dL3mvOli_-0y"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config1 = {\n",
        "    'num_layers': 6,\n",
        "    'd_model': 512,\n",
        "    'num_heads': 8,\n",
        "    'dff': 2048,\n",
        "    'dropout': 0.1,\n",
        "    'seq_len': 100\n",
        "}\n",
        "vocab_source_length = 30000\n",
        "vocab_target_length = 30000\n",
        "\n",
        "model = get_model(config1, vocab_source_length, vocab_target_length)"
      ],
      "metadata": {
        "id": "XIYDMvLjHnlB"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "vocab_source_length = tokenizer_source.get_vocab_size()\n",
        "vocab_target_length = tokenizer_target.get_vocab_size()\n",
        "print(f'vocab source len {vocab_source_length}')\n",
        "print(f'vocab target len {vocab_target_length}')\n",
        "model = get_model(config, vocab_source_length, vocab_target_length)\n",
        "writer = SummaryWriter(config['experiment_name'])\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer_target.token_to_id(\"[PAD]\"),\n",
        "                                      label_smoothing=0.1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], eps=1e-9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkI54G42Ix34",
        "outputId": "abef7d47-cf34-4240-b337-b74dccd6596f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "vocab source len 15698\n",
            "vocab target len 22463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Define loss function and optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer_target.token_to_id(\"[PAD]\"),\n",
        "                                          label_smoothing=0.1).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0"
      ],
      "metadata": {
        "id": "B0AXVZPELvLa"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  if config['preload']:\n",
        "      model_filename = get_weights_file_path(config, config['preload'])\n",
        "      print(\" Pre-Loading model\", model_filename)\n",
        "      state = torch.load(model_filename)\n",
        "      initial_epoch = state['epoch'] + 1\n",
        "      optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "      global_step = state['global_step']"
      ],
      "metadata": {
        "id": "Bshx6esZLvPV"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5MRhmHbLbxe",
        "outputId": "63431982-9bc6-48e6-c38d-ed3595f29e77"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.tensorboard.writer.SummaryWriter at 0x7bf8a7baba00>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How the Masks Work\n",
        "# Encoder Self-Attention:\n",
        "\n",
        "# Applies the encoder_mask to attention scores, ensuring padding tokens don't contribute.\n",
        "# Decoder Self-Attention:\n",
        "\n",
        "# Applies the decoder_mask to attention scores, ensuring both:\n",
        "# Causal masking: Only attends to previous and current tokens.\n",
        "# Padding masking: Ignores padding tokens.\n",
        "# Encoder-Decoder Attention:\n",
        "\n",
        "# Uses the encoder_mask to prevent attention to padding tokens in the source sequence.\n",
        "\n",
        "# Mask Shape: (Batch, 1, 1, Seq_Len)\n",
        "# Example Mask (Seq_Len = 5):\n",
        "# [1, 1, 1, 0, 0] -> Indicates valid tokens (1) and padding tokens (0).\n",
        "\n",
        "\n",
        "# Mask Shape: (Batch, 1, Seq_Len, Seq_Len)\n",
        "# Example Mask (Seq_Len = 5):\n",
        "# [[1, 0, 0, 0, 0],\n",
        "#  [1, 1, 0, 0, 0],\n",
        "#  [1, 1, 1, 0, 0],\n",
        "#  [1, 1, 1, 1, 0],\n",
        "#  [1, 1, 1, 1, 1]] -> Prevents attending to future tokens."
      ],
      "metadata": {
        "id": "9p2GzbPg4TYQ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Output\n",
        "encoder_output = model.encode(encoder_input, encoder_mask)  # (Batch, Seq_Len, d_model)\n",
        "\n",
        "Input:\n",
        "  - encoder_input: The tokenized source text ((Batch, Seq_Len)).\n",
        "  - encoder_mask: A mask to ignore padding tokens in the encoder.\n",
        "\n",
        "Process:\n",
        "The encoder processes the input sequence and generates contextualized embeddings for each token.\n",
        "Output:\n",
        "encoder_output: A tensor containing contextualized embeddings for each position in the sequence.\n",
        "\n",
        "Shape: (Batch, Seq_Len, d_model)\n",
        "\n",
        "Batch: Number of sequences in the batch.\n",
        "\n",
        "Seq_Len: Length of each sequence.\n",
        "\n",
        "d_model: Dimension of the model's embeddings.\n",
        "\n",
        "# Decoder Output\n",
        "decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)  # (Batch, Seq_Len, d_model)\n",
        "\n",
        "Input:\n",
        "\n",
        "- decoder_input: The tokenized target text up to the current step during training ((Batch, Seq_Len)).\n",
        "\n",
        "- encoder_output: The output of the encoder ((Batch, Seq_Len, d_model)).\n",
        "\n",
        "- encoder_mask: Ensures the decoder does not attend to padding tokens in the encoder output.\n",
        "\n",
        "- decoder_mask: Ensures causal masking (no attending to future tokens) and ignores padding tokens in the target sequence.\n",
        "\n",
        "Process:\n",
        "\n",
        "The decoder attends to the encoder_output and its own decoder_input to produce predictions for the target sequence.\n",
        "Output:\n",
        "\n",
        "- decoder_output: A tensor containing contextualized embeddings for the decoder's current prediction.\n",
        "\n",
        "- Shape: (Batch, Seq_Len, d_model)\n",
        "\n",
        "# Projection to Vocabulary\n",
        "projection_output = model.project(decoder_output)  # (Batch, Seq_Len, target_vocab_size)\n",
        "\n",
        "Input:\n",
        "\n",
        "- decoder_output: Contextualized embeddings from the decoder.\n",
        "Process:\n",
        "\n",
        "- Applies a linear transformation to project the decoder output into a probability distribution over the target vocabulary.\n",
        "Output:\n",
        "\n",
        "- projection_output: Logits representing the likelihood of each token in the target vocabulary.\n",
        "- Shape: (Batch, Seq_Len, target_vocab_size)\n",
        "- target_vocab_size: Number of tokens in the target vocabulary.\n",
        "\n",
        "# Target Labels\n",
        "\n",
        "target_label = batch['target_label'].to(device)  # (Batch, Seq_Len)\n",
        "\n",
        "Input:\n",
        "\n",
        "- batch['target_label']: Ground truth token indices for the target sequence.\n",
        "Process:\n",
        "\n",
        "- Moves the target labels to the same device (CPU/GPU) as the model for loss computation.\n",
        "\n",
        "Output:\n",
        "\n",
        "- target_label: Token indices for the target sequence.\n",
        "- Shape: (Batch, Seq_Len)"
      ],
      "metadata": {
        "id": "64yySehS5akx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation(model, validation_dataset, tokenizer_source, tokenizer_target, max_length, device, print_msg,\n",
        "                   global_state, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    # source_texts = []\n",
        "    # expected = []\n",
        "    # predicted = []\n",
        "\n",
        "    # Size of the control window( just us a default value)\n",
        "    console_width = 80\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_dataset:\n",
        "            count += 1\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "\n",
        "            assert encoder_input.size(0) == 1, \" Batch size must be 1 for validation\"\n",
        "\n",
        "            model_output = greedy_decode(model, encoder_input, encoder_mask, tokenizer_source, tokenizer_target,\n",
        "                                         max_length,\n",
        "                                         device)\n",
        "\n",
        "            source_text = batch['source_text'][0]\n",
        "            target_text = batch['target_text'][0]\n",
        "\n",
        "            model_output_text = tokenizer_target.decode(model_output.detach().cpu().numpy())\n",
        "\n",
        "            # source_texts.append(source_text)\n",
        "            # expected.append(target_text)\n",
        "            # predicted.append(model_output_text)\n",
        "\n",
        "            # Print to the console\n",
        "\n",
        "            print_msg('-' * console_width)\n",
        "            print_msg(f'SOURCE{source_text}')\n",
        "            print_msg(f'TARGET{target_text}')\n",
        "            print_msg(f'PREDICT{model_output_text}')\n",
        "\n",
        "            if count == num_examples:\n",
        "                break"
      ],
      "metadata": {
        "id": "T2SYiORaomNa"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(config):\n",
        "    # Define the device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Get training data, validation data, source tokenizer, and target tokenizer\n",
        "    training_dataloader, validation_dataloader, tokenizer_source, tokenizer_target = get_dataset(config)\n",
        "\n",
        "    # Get vocabulary sizes for source and target languages from tokenizers\n",
        "    vocab_source_length = tokenizer_source.get_vocab_size()\n",
        "    vocab_target_length = tokenizer_target.get_vocab_size()\n",
        "\n",
        "    # Build the transformer model\n",
        "    model = get_model(config, vocab_source_length, vocab_target_length)\n",
        "\n",
        "    # Tensorboard # writes data into a specified directory, and TensorBoard reads from that directory to generate visualizations.\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer_target.token_to_id(\"[PAD]\"),\n",
        "                                          label_smoothing=0.1).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config, config['preload'])\n",
        "        print(\" Pre-Loading model\", model_filename)\n",
        "        state = torch.load(model_filename)\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = config['num_epochs']\n",
        "    for epoch in range(initial_epoch, num_epochs):\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(training_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in training_dataloader:\n",
        "            model.train()\n",
        "            # The shapes of the encoder_mask and decoder_mask are specific to how the attention mechanism in a Transformer model works.\n",
        "            # This represents the tokenized sequences for the source text. Each sequence has a length of Seq_Len.\n",
        "            encoder_input = batch['encoder_input'].to(device)  # (Batch , Seq_Len)\n",
        "            decoder_input = batch['decoder_input'].to(device)  # (Batch , Seq_Len)\n",
        "\n",
        "            # The mask for the encoder is applied during self-attention in the encoder.\n",
        "            # The attention score matrix has a shape of (Batch, Num_Heads, Seq_Len, Seq_Len)\n",
        "            # It ensures the model only attends to valid tokens (e.g., ignores padding tokens).\n",
        "            # The shape (Batch, 1, 1, Seq_Len) is designed to broadcast correctly when computing attention scores:\n",
        "            # Doesn't need to consider future tokens since the encoder processes the entire source sequence at once.\n",
        "            encoder_mask = batch['encoder_mask'].to(device)  # (Batch ,1 ,1 ,Seq_Len)\n",
        "            # Needs to consider causal masking to prevent future tokens from being attended to.\n",
        "            # Requires a mask of shape (Batch, 1, Seq_Len, Seq_Len) to apply both causal and padding masks for self-attention.\n",
        "            # Only attends to tokens that have been generated up to the current position\n",
        "            # Ignores padding tokens in the target text.\n",
        "            decoder_mask = batch['decoder_mask'].to(device)  # (Batch ,1 ,Seq_len ,Seq_Len)\n",
        "\n",
        "            print(\"Encoder Input Shape:\", encoder_input.shape)\n",
        "            print(\"Decoder Input Shape:\", decoder_input.shape)\n",
        "            print(\"Encoder Mask Shape:\", encoder_mask.shape)\n",
        "            print(\"Decoder Mask Shape:\", decoder_mask.shape)\n",
        "\n",
        "            # Run the tensors through the transformer\n",
        "\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)  # (Batch , Seq_Len, d_model)\n",
        "            decoder_output = model.decode(decoder_input, encoder_output, encoder_mask,\n",
        "                                          decoder_mask)  # (Batch  Seq_Len, d_model)\n",
        "            projection_output = model.project(decoder_output)  # (Batch, Seq_Len, target_vocab_size)\n",
        "\n",
        "            target_label = batch['target_label'].to(device)  # (Batch, Seq_Len)\n",
        "\n",
        "            # Calculate the loss\n",
        "            # Flatten the projection_output and target_label tensors for the CrossEntropyLoss\n",
        "            # Projection output shape after view: (Batch * Seq_Len, target_vocab_size)\n",
        "            # Target label shape after view: (Batch * Seq_Len)\n",
        "            loss = criterion(projection_output.view(-1, projection_output.shape[-1]), target_label.view(-1))\n",
        "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backpropagation and optimization step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "            # Increment the global step count\n",
        "            global_step += 1\n",
        "\n",
        "        run_validation(model, validation_dataloader, tokenizer_source, tokenizer_target, config['seq_len'],\n",
        "                       device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Calculate the average loss for the epoch\n",
        "        avg_loss = total_loss / len(training_dataloader)\n",
        "\n",
        "        # Log the loss to Tensorboard\n",
        "        writer.add_scalar('Training Loss', avg_loss, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Print epoch info\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save the trained model\n",
        "\n",
        "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "            'global_step': global_step\n",
        "        }, model_filename)"
      ],
      "metadata": {
        "id": "xSgmvWoNHzIT"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, source, encoder_mask, tokenizer_source, tokenizer_target, max_length, device):\n",
        "    sos_idx = tokenizer_target.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_target.token_to_id('[EOS]')\n",
        "\n",
        "    # Pre-compute the encoder output and reuse it for every token we get from the decoder\n",
        "    encoder_output = model.encode(source, encoder_mask)\n",
        "    # Initialize the decoder input with sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(encoder_mask).to(device)\n",
        "\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_length:\n",
        "            break\n",
        "\n",
        "        # Build the mask for the target ( decoder input )\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n",
        "\n",
        "        # Calculate the output of the decoder\n",
        "        decoder_output = model.decode(decoder_input, encoder_output, encoder_mask,\n",
        "                                      decoder_mask)\n",
        "\n",
        "        # Get the next token\n",
        "        probabilities = model.project(decoder_output[:, -1])\n",
        "\n",
        "        # Select the token with the max probability (because it is greedy search)\n",
        "        _, next_word = torch.max(probabilities, dim=1)\n",
        "\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)],\n",
        "                                  dim=1)\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "hsWoJlNtub-_",
        "outputId": "ec2bf9e6-5ae1-460d-a606-91d640ac9197"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dataset.bilingual'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-09ea44524f9f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilingual\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBilingualDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset.bilingual'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9Pp7KUvoi6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model"
      ],
      "metadata": {
        "id": "Zf60wabCBmge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"InputEmbeddings - Input x shape:\", x.shape)\n",
        "        embeddings = self.word_embeddings(x) * math.sqrt(self.d_model)\n",
        "        print(\"InputEmbeddings - Output embeddings shape:\", embeddings.shape)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"PositionalEncoding - Input x shape:\", x.shape)\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        print(\"PositionalEncoding - Output x shape:\", x.shape)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class LayerNormalisation(nn.Module):\n",
        "    def __init__(self, epsilon=1e-6):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dff = dff\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dff)\n",
        "        self.linear2 = nn.Linear(dff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"FeedForward - Input x shape:\", x.shape)\n",
        "        x = self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
        "        print(\"FeedForward - Output x shape:\", x.shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.attention_scores = None\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.d_k = d_model // self.num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.layer_norm1 = LayerNormalisation()\n",
        "        self.layer_norm2 = LayerNormalisation()\n",
        "        self.layer_norm3 = LayerNormalisation()\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(q, k, v, mask, dropout):\n",
        "        d_k = q.shape[-1]\n",
        "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1)  # (Batch, num_heads, Seq_Len,  Seq_Len)\n",
        "\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        attn = torch.matmul(attention_scores, v)  # (Batch, num_heads, Seq_Len, d_k)\n",
        "        return attn, attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "\n",
        "        q = self.wq(q)\n",
        "\n",
        "        k = self.wk(k)\n",
        "\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = q.view(q.shape[0], q.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        k = k.view(k.shape[0], k.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        v = v.view(v.shape[0], v.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, self.attention_scores = MultiHeadAttention.attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "        x = self.wo(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = LayerNormalisation()\n",
        "\n",
        "    def forward(self, x, sub_layer):\n",
        "        return x + self.dropout(sub_layer(self.layer_norm(x)))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.dff = dff  # Feed Forward Neural Network Output Size\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = FeedForward(d_model, dff, dropout)\n",
        "        self.residual_mha = ResidualConnection(dropout)\n",
        "        self.residual_ffn = ResidualConnection(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Multi-Head Attention sub-layer\n",
        "        attn_output = self.residual_mha(x, lambda x: self.mha(x, x, x, mask))\n",
        "\n",
        "        # FeedForward sub-layer\n",
        "        ffn_output = self.residual_ffn(attn_output, self.ffn)\n",
        "\n",
        "        return ffn_output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer = nn.ModuleList([EncoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)])\n",
        "        self.layer_norm = LayerNormalisation()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.layer[i](x, mask)\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff  # Feed Forward Neural Network Output Size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = FeedForward(d_model, dff, dropout)\n",
        "        self.residual_mha = ResidualConnection(dropout)\n",
        "        self.residual_cross_mha = ResidualConnection(dropout)\n",
        "        self.residual_ffn = ResidualConnection(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
        "        # Multi-Head Attention sub-layer\n",
        "        attn_output = self.residual_mha(x, lambda x: self.mha(x, x, x, target_mask))\n",
        "\n",
        "        # Cross-Attention sub-layer\n",
        "        cross_attn_output = self.residual_cross_mha(attn_output,\n",
        "                                                    lambda x: self.mha(x, encoder_output, encoder_output, source_mask))\n",
        "\n",
        "        # FeedForward sub-layer\n",
        "        ffn_output = self.residual_ffn(cross_attn_output, self.ffn)\n",
        "\n",
        "        return ffn_output\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer = nn.ModuleList([DecoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)])\n",
        "        self.layer_norm = LayerNormalisation()\n",
        "\n",
        "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.layer[i](x, encoder_output, source_mask, target_mask)\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model, vocabulary_size):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.projection = nn.Linear(d_model, vocabulary_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (Batch, Seq_Len, D_Model) -->( Batch, Seq_Len, Vocab_Size)\n",
        "        return torch.log_softmax(self.projection(x), dim=-1)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout, source_embeddings, target_embeddings,\n",
        "                 source_pos_encodings, target_pos_encodings, vocabulary_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, dropout)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, dropout)\n",
        "        self.projection = ProjectionLayer(d_model, vocabulary_size)\n",
        "        self.source_embeddings = source_embeddings\n",
        "        self.target_embeddings = target_embeddings\n",
        "        self.source_pos_encodings = source_pos_encodings\n",
        "        self.target_pos_encodings = target_pos_encodings\n",
        "\n",
        "    def encode(self, source_input, source_mask):\n",
        "        # Embedding and positional encoding for source inputs\n",
        "        source_embedded = self.source_embeddings(source_input)\n",
        "        source_embedded = self.source_pos_encodings(source_embedded)\n",
        "        # Pass source input through the encoder\n",
        "        encoder_output = self.encoder(source_embedded, source_mask)\n",
        "        return encoder_output\n",
        "\n",
        "    def decode(self, target_input, encoder_output, source_mask, target_mask):\n",
        "        # Embedding and positional encoding for target inputs\n",
        "        target_embedded = self.target_embeddings(target_input)\n",
        "        target_embedded = self.target_pos_encodings(target_embedded)\n",
        "        # Pass target input through the decoder\n",
        "        decoder_output = self.decoder(target_embedded, encoder_output, source_mask, target_mask)\n",
        "        return decoder_output\n",
        "\n",
        "    def project(self, decoder_output):\n",
        "        # Project the decoder output to the vocabulary size\n",
        "        output_logits = self.projection(decoder_output)\n",
        "        return output_logits\n",
        "\n",
        "\n",
        "def build_transformer(num_layers, d_model, num_heads, dff, dropout, source_vocab_size, target_vocab_size,\n",
        "                      max_seq_len):\n",
        "    # Create embeddings and positional encodings\n",
        "    source_embeddings = InputEmbeddings(d_model, source_vocab_size)\n",
        "    target_embeddings = InputEmbeddings(d_model, target_vocab_size)\n",
        "    source_pos_encodings = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "    target_pos_encodings = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "\n",
        "    # Create the Transformer model\n",
        "    transformer = Transformer(num_layers, d_model, num_heads, dff, dropout,\n",
        "                              source_embeddings, target_embeddings,\n",
        "                              source_pos_encodings, target_pos_encodings, target_vocab_size)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "P31X_9C3d-5x"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RN3Bp33dBpE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}